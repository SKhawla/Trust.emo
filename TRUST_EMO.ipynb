{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.pyplot import specgram\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding\n",
    "from keras.layers import LSTM\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Input, Flatten, Dropout, Activation\n",
    "from keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mylist= os.listdir('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(mylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03-01-01-01-01-01-12.wav\n"
     ]
    }
   ],
   "source": [
    "print(mylist[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "01\n"
     ]
    }
   ],
   "source": [
    "print(mylist[40][6:-16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, sampling_rate = librosa.load('data/neutral1.wav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PolyCollection at 0x1c903ed8860>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import glob \n",
    "\n",
    "plt.figure(figsize=(15, 5))\n",
    "librosa.display.waveplot(data, sr=sampling_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\matplotlib\\image.py:855: UserWarning: Attempting to set identical left==right results\n",
      "in singular transformations; automatically expanding.\n",
      "left=-0.5, right=-0.5\n",
      "  self.axes.set_xlim((xmin, xmax), auto=None)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.io.wavfile\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "\n",
    "#sr,x = scipy.io.wavfile.read('data/neutral1.wav',mmap=True)\n",
    "import io\n",
    "from scipy.io.wavfile import read\n",
    "import wave\n",
    "\n",
    "def normalize_wav(input_file, output_file):\n",
    "    with wave.open(input_file, \"rb\") as r_wav, wave.open(output_file, \"wb\") as w_wav:\n",
    "        w_wav.setparams(r_wav.getparams())\n",
    "        w_wav.writeframes(r_wav.readframes(w_wav.getnframes()))\n",
    "\n",
    "buff = io.BytesIO()\n",
    "normalize_wav('data/neutral1.wav', buff)\n",
    "buff.seek(0)\n",
    "sr,x = read(buff)\n",
    "\n",
    "## Parameters: 10ms step, 30ms window\n",
    "nstep = int(sr * 0.01)\n",
    "nwin  = int(sr * 0.03)\n",
    "nfft = nwin\n",
    "\n",
    "window = np.hamming(nwin)\n",
    "\n",
    "## will take windows x[n1:n2].  generate\n",
    "## and loop over n2 such that all frames\n",
    "## fit within the waveform\n",
    "nn = range(nwin, len(x), nstep)\n",
    "\n",
    "X = np.zeros( (len(nn), nfft//2) )\n",
    "\n",
    "for i,n in enumerate(nn):\n",
    "    xseg = x[n-nwin:n]\n",
    "    z = np.fft.fft(window * xseg, nfft)\n",
    "    X[i,:] = np.log(np.abs(z[:nfft//2]))\n",
    "\n",
    "plt.imshow(X.T, interpolation='nearest',\n",
    "    origin='lower',\n",
    "    aspect='auto')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "feeling_list=[]\n",
    "for item in mylist:\n",
    "    if item[6:-16]=='02' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_calm')\n",
    "    elif item[6:-16]=='02' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_calm')\n",
    "    elif item[6:-16]=='03' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_happy')\n",
    "    elif item[6:-16]=='03' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_happy')\n",
    "    elif item[6:-16]=='04' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_sad')\n",
    "    elif item[6:-16]=='04' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_sad')\n",
    "    elif item[6:-16]=='05' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_angry')\n",
    "    elif item[6:-16]=='05' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_angry')\n",
    "    elif item[6:-16]=='06' and int(item[18:-4])%2==0:\n",
    "        feeling_list.append('female_fearful')\n",
    "    elif item[6:-16]=='06' and int(item[18:-4])%2==1:\n",
    "        feeling_list.append('male_fearful')\n",
    "    elif item[:1]=='a':\n",
    "        feeling_list.append('male_angry')\n",
    "    elif item[:1]=='f':\n",
    "        feeling_list.append('male_fearful')\n",
    "    elif item[:1]=='h':\n",
    "        feeling_list.append('male_happy')\n",
    "    #elif item[:1]=='n':\n",
    "        #feeling_list.append('neutral')\n",
    "    elif item[:2]=='sa':\n",
    "        feeling_list.append('male_sad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.DataFrame(feeling_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "0  female_calm\n",
       "1    male_calm\n",
       "2  female_calm\n",
       "3    male_calm\n",
       "4  female_calm\n",
       "5    male_calm\n",
       "6  female_calm\n",
       "7    male_calm\n",
       "8  female_calm\n",
       "9    male_calm"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['feature'])\n",
    "bookmark=0\n",
    "for index,y in enumerate(mylist):\n",
    "    if mylist[index][6:-16]!='01' and mylist[index][6:-16]!='07' and mylist[index][6:-16]!='08' and mylist[index][:2]!='su' and mylist[index][:1]!='n' and mylist[index][:1]!='d':\n",
    "        X, sample_rate = librosa.load('data/'+y, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "        sample_rate = np.array(sample_rate)\n",
    "        mfccs = np.mean(librosa.feature.mfcc(y=X, \n",
    "                                            sr=sample_rate, \n",
    "                                            n_mfcc=13),\n",
    "                        axis=0)\n",
    "        feature = mfccs\n",
    "        #[float(i) for i in feature]\n",
    "        #feature1=feature[:135]\n",
    "        df.loc[bookmark] = [feature]\n",
    "        bookmark=bookmark+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-65.70765, -65.70765, -63.11472, -61.518997, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-65.4825, -65.4825, -65.4825, -65.4825, -65.4...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-64.52845, -64.52845, -64.52845, -64.52845, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-62.36431, -59.93473, -61.869602, -67.495766,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-71.288345, -71.288345, -71.288345, -71.28834...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature\n",
       "0  [-65.70765, -65.70765, -63.11472, -61.518997, ...\n",
       "1  [-65.4825, -65.4825, -65.4825, -65.4825, -65.4...\n",
       "2  [-64.52845, -64.52845, -64.52845, -64.52845, -...\n",
       "3  [-62.36431, -59.93473, -61.869602, -67.495766,...\n",
       "4  [-71.288345, -71.288345, -71.288345, -71.28834..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.DataFrame(df['feature'].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.concat([df3,labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnewdf = newdf.rename(index=str, columns={\"0\": \"label\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-65.707649</td>\n",
       "      <td>-65.707649</td>\n",
       "      <td>-63.114719</td>\n",
       "      <td>-61.518997</td>\n",
       "      <td>-61.097134</td>\n",
       "      <td>-63.424595</td>\n",
       "      <td>-63.720058</td>\n",
       "      <td>-56.854614</td>\n",
       "      <td>-55.168972</td>\n",
       "      <td>-54.640007</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.792141</td>\n",
       "      <td>-40.613159</td>\n",
       "      <td>-41.209202</td>\n",
       "      <td>-41.439194</td>\n",
       "      <td>-43.994286</td>\n",
       "      <td>-49.399620</td>\n",
       "      <td>-50.591599</td>\n",
       "      <td>-49.144062</td>\n",
       "      <td>-48.705647</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>-65.482498</td>\n",
       "      <td>...</td>\n",
       "      <td>-31.346554</td>\n",
       "      <td>-34.310772</td>\n",
       "      <td>-35.800705</td>\n",
       "      <td>-35.936115</td>\n",
       "      <td>-37.631844</td>\n",
       "      <td>-40.119411</td>\n",
       "      <td>-41.662903</td>\n",
       "      <td>-41.323643</td>\n",
       "      <td>-40.710770</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-64.528450</td>\n",
       "      <td>-65.928223</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.674301</td>\n",
       "      <td>-48.596077</td>\n",
       "      <td>-47.602749</td>\n",
       "      <td>-43.049198</td>\n",
       "      <td>-42.659538</td>\n",
       "      <td>-43.188564</td>\n",
       "      <td>-44.001240</td>\n",
       "      <td>-43.610100</td>\n",
       "      <td>-44.698250</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-62.364311</td>\n",
       "      <td>-59.934731</td>\n",
       "      <td>-61.869602</td>\n",
       "      <td>-67.495766</td>\n",
       "      <td>-71.071815</td>\n",
       "      <td>-65.679817</td>\n",
       "      <td>-63.394402</td>\n",
       "      <td>-65.503349</td>\n",
       "      <td>-61.856636</td>\n",
       "      <td>-60.005424</td>\n",
       "      <td>...</td>\n",
       "      <td>-39.071320</td>\n",
       "      <td>-41.897129</td>\n",
       "      <td>-40.865433</td>\n",
       "      <td>-38.290604</td>\n",
       "      <td>-36.372398</td>\n",
       "      <td>-37.915783</td>\n",
       "      <td>-40.026127</td>\n",
       "      <td>-43.383770</td>\n",
       "      <td>-43.965401</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-71.288345</td>\n",
       "      <td>-71.288345</td>\n",
       "      <td>-71.288345</td>\n",
       "      <td>-71.288345</td>\n",
       "      <td>-71.288345</td>\n",
       "      <td>-71.288345</td>\n",
       "      <td>-71.288345</td>\n",
       "      <td>-71.288345</td>\n",
       "      <td>-71.288345</td>\n",
       "      <td>-71.288345</td>\n",
       "      <td>...</td>\n",
       "      <td>-45.439762</td>\n",
       "      <td>-45.199463</td>\n",
       "      <td>-43.801682</td>\n",
       "      <td>-44.768456</td>\n",
       "      <td>-46.546829</td>\n",
       "      <td>-45.922283</td>\n",
       "      <td>-46.734531</td>\n",
       "      <td>-42.166695</td>\n",
       "      <td>-37.963127</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "0 -65.707649 -65.707649 -63.114719 -61.518997 -61.097134 -63.424595   \n",
       "1 -65.482498 -65.482498 -65.482498 -65.482498 -65.482498 -65.482498   \n",
       "2 -64.528450 -64.528450 -64.528450 -64.528450 -64.528450 -64.528450   \n",
       "3 -62.364311 -59.934731 -61.869602 -67.495766 -71.071815 -65.679817   \n",
       "4 -71.288345 -71.288345 -71.288345 -71.288345 -71.288345 -71.288345   \n",
       "\n",
       "         6          7          8          9    ...        207        208  \\\n",
       "0 -63.720058 -56.854614 -55.168972 -54.640007  ... -39.792141 -40.613159   \n",
       "1 -65.482498 -65.482498 -65.482498 -65.482498  ... -31.346554 -34.310772   \n",
       "2 -64.528450 -64.528450 -64.528450 -65.928223  ... -48.674301 -48.596077   \n",
       "3 -63.394402 -65.503349 -61.856636 -60.005424  ... -39.071320 -41.897129   \n",
       "4 -71.288345 -71.288345 -71.288345 -71.288345  ... -45.439762 -45.199463   \n",
       "\n",
       "         209        210        211        212        213        214  \\\n",
       "0 -41.209202 -41.439194 -43.994286 -49.399620 -50.591599 -49.144062   \n",
       "1 -35.800705 -35.936115 -37.631844 -40.119411 -41.662903 -41.323643   \n",
       "2 -47.602749 -43.049198 -42.659538 -43.188564 -44.001240 -43.610100   \n",
       "3 -40.865433 -38.290604 -36.372398 -37.915783 -40.026127 -43.383770   \n",
       "4 -43.801682 -44.768456 -46.546829 -45.922283 -46.734531 -42.166695   \n",
       "\n",
       "         215          0    \n",
       "0 -48.705647  female_calm  \n",
       "1 -40.710770    male_calm  \n",
       "2 -44.698250  female_calm  \n",
       "3 -43.965401    male_calm  \n",
       "4 -37.963127  female_calm  \n",
       "\n",
       "[5 rows x 217 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnewdf[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>317</th>\n",
       "      <td>-44.929764</td>\n",
       "      <td>-45.915077</td>\n",
       "      <td>-48.502754</td>\n",
       "      <td>-50.057381</td>\n",
       "      <td>-50.445156</td>\n",
       "      <td>-51.035885</td>\n",
       "      <td>-51.243423</td>\n",
       "      <td>-51.221672</td>\n",
       "      <td>-50.785576</td>\n",
       "      <td>-50.070919</td>\n",
       "      <td>...</td>\n",
       "      <td>-24.733873</td>\n",
       "      <td>-22.042908</td>\n",
       "      <td>-21.389210</td>\n",
       "      <td>-22.735104</td>\n",
       "      <td>-21.326191</td>\n",
       "      <td>-19.974070</td>\n",
       "      <td>-19.913639</td>\n",
       "      <td>-18.530849</td>\n",
       "      <td>-14.574443</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>877</th>\n",
       "      <td>-50.922508</td>\n",
       "      <td>-46.728203</td>\n",
       "      <td>-45.134315</td>\n",
       "      <td>-45.604366</td>\n",
       "      <td>-45.114830</td>\n",
       "      <td>-44.592960</td>\n",
       "      <td>-45.553383</td>\n",
       "      <td>-45.842388</td>\n",
       "      <td>-45.717918</td>\n",
       "      <td>-46.047836</td>\n",
       "      <td>...</td>\n",
       "      <td>-38.503208</td>\n",
       "      <td>-39.523758</td>\n",
       "      <td>-40.019905</td>\n",
       "      <td>-41.528805</td>\n",
       "      <td>-42.962498</td>\n",
       "      <td>-42.645603</td>\n",
       "      <td>-42.775723</td>\n",
       "      <td>-42.864445</td>\n",
       "      <td>-42.943764</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>-53.070736</td>\n",
       "      <td>-51.800091</td>\n",
       "      <td>-50.760410</td>\n",
       "      <td>-52.047302</td>\n",
       "      <td>-51.221233</td>\n",
       "      <td>-51.671104</td>\n",
       "      <td>-51.556725</td>\n",
       "      <td>-52.577854</td>\n",
       "      <td>-53.201706</td>\n",
       "      <td>-52.130268</td>\n",
       "      <td>...</td>\n",
       "      <td>-53.811703</td>\n",
       "      <td>-54.319889</td>\n",
       "      <td>-55.268959</td>\n",
       "      <td>-52.052856</td>\n",
       "      <td>-50.433273</td>\n",
       "      <td>-53.498554</td>\n",
       "      <td>-53.312832</td>\n",
       "      <td>-52.798386</td>\n",
       "      <td>-51.479801</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>-40.588501</td>\n",
       "      <td>-40.581081</td>\n",
       "      <td>-40.605572</td>\n",
       "      <td>-40.649166</td>\n",
       "      <td>-40.645645</td>\n",
       "      <td>-40.654915</td>\n",
       "      <td>-40.655987</td>\n",
       "      <td>-40.645638</td>\n",
       "      <td>-40.632103</td>\n",
       "      <td>-40.652580</td>\n",
       "      <td>...</td>\n",
       "      <td>-37.957699</td>\n",
       "      <td>-37.451443</td>\n",
       "      <td>-39.119526</td>\n",
       "      <td>-39.735306</td>\n",
       "      <td>-40.298763</td>\n",
       "      <td>-40.656857</td>\n",
       "      <td>-40.656857</td>\n",
       "      <td>-40.656857</td>\n",
       "      <td>-40.656857</td>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>-55.193134</td>\n",
       "      <td>-55.723881</td>\n",
       "      <td>-56.979527</td>\n",
       "      <td>-60.773300</td>\n",
       "      <td>-55.671585</td>\n",
       "      <td>-53.729595</td>\n",
       "      <td>-53.668484</td>\n",
       "      <td>-53.156345</td>\n",
       "      <td>-54.001930</td>\n",
       "      <td>-55.969082</td>\n",
       "      <td>...</td>\n",
       "      <td>-52.854710</td>\n",
       "      <td>-49.178703</td>\n",
       "      <td>-49.217075</td>\n",
       "      <td>-50.817707</td>\n",
       "      <td>-50.598225</td>\n",
       "      <td>-50.945152</td>\n",
       "      <td>-50.485638</td>\n",
       "      <td>-51.919376</td>\n",
       "      <td>-52.982174</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>-41.152237</td>\n",
       "      <td>-41.695740</td>\n",
       "      <td>-39.584538</td>\n",
       "      <td>-38.757969</td>\n",
       "      <td>-38.952866</td>\n",
       "      <td>-40.042168</td>\n",
       "      <td>-40.949989</td>\n",
       "      <td>-40.641487</td>\n",
       "      <td>-41.340652</td>\n",
       "      <td>-42.487789</td>\n",
       "      <td>...</td>\n",
       "      <td>-45.526016</td>\n",
       "      <td>-47.148186</td>\n",
       "      <td>-46.474819</td>\n",
       "      <td>-44.839092</td>\n",
       "      <td>-46.956478</td>\n",
       "      <td>-47.138290</td>\n",
       "      <td>-48.784267</td>\n",
       "      <td>-46.908516</td>\n",
       "      <td>-45.789280</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.487358</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.648575</td>\n",
       "      <td>-39.472900</td>\n",
       "      <td>-39.716457</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>...</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>-40.758728</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>-50.683449</td>\n",
       "      <td>-49.582844</td>\n",
       "      <td>-49.573147</td>\n",
       "      <td>-49.530697</td>\n",
       "      <td>-51.778389</td>\n",
       "      <td>-49.248268</td>\n",
       "      <td>-49.240765</td>\n",
       "      <td>-49.972069</td>\n",
       "      <td>-51.259541</td>\n",
       "      <td>-50.161118</td>\n",
       "      <td>...</td>\n",
       "      <td>-50.108318</td>\n",
       "      <td>-50.828201</td>\n",
       "      <td>-51.525124</td>\n",
       "      <td>-50.562870</td>\n",
       "      <td>-49.983562</td>\n",
       "      <td>-52.057095</td>\n",
       "      <td>-50.964302</td>\n",
       "      <td>-50.562965</td>\n",
       "      <td>-51.980816</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>-72.317825</td>\n",
       "      <td>-72.317825</td>\n",
       "      <td>-72.317825</td>\n",
       "      <td>-72.317825</td>\n",
       "      <td>-72.317825</td>\n",
       "      <td>-72.317825</td>\n",
       "      <td>-72.317825</td>\n",
       "      <td>-72.317825</td>\n",
       "      <td>-72.317825</td>\n",
       "      <td>-72.317825</td>\n",
       "      <td>...</td>\n",
       "      <td>-64.586044</td>\n",
       "      <td>-64.523933</td>\n",
       "      <td>-64.174210</td>\n",
       "      <td>-66.311821</td>\n",
       "      <td>-63.141403</td>\n",
       "      <td>-60.602013</td>\n",
       "      <td>-63.572910</td>\n",
       "      <td>-64.221634</td>\n",
       "      <td>-63.324245</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>514</th>\n",
       "      <td>-46.456718</td>\n",
       "      <td>-47.024212</td>\n",
       "      <td>-46.142891</td>\n",
       "      <td>-46.262913</td>\n",
       "      <td>-46.480347</td>\n",
       "      <td>-46.620770</td>\n",
       "      <td>-47.260395</td>\n",
       "      <td>-46.667107</td>\n",
       "      <td>-47.757446</td>\n",
       "      <td>-46.490688</td>\n",
       "      <td>...</td>\n",
       "      <td>-45.584679</td>\n",
       "      <td>-43.598026</td>\n",
       "      <td>-46.936806</td>\n",
       "      <td>-47.840889</td>\n",
       "      <td>-48.035343</td>\n",
       "      <td>-47.782524</td>\n",
       "      <td>-45.367886</td>\n",
       "      <td>-45.400936</td>\n",
       "      <td>-47.511478</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows  217 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0          1          2          3          4          5    \\\n",
       "317 -44.929764 -45.915077 -48.502754 -50.057381 -50.445156 -51.035885   \n",
       "877 -50.922508 -46.728203 -45.134315 -45.604366 -45.114830 -44.592960   \n",
       "277 -53.070736 -51.800091 -50.760410 -52.047302 -51.221233 -51.671104   \n",
       "304 -40.588501 -40.581081 -40.605572 -40.649166 -40.645645 -40.654915   \n",
       "102 -55.193134 -55.723881 -56.979527 -60.773300 -55.671585 -53.729595   \n",
       "367 -41.152237 -41.695740 -39.584538 -38.757969 -38.952866 -40.042168   \n",
       "741 -40.758728 -40.758728 -40.487358 -40.758728 -40.648575 -39.472900   \n",
       "279 -50.683449 -49.582844 -49.573147 -49.530697 -51.778389 -49.248268   \n",
       "460 -72.317825 -72.317825 -72.317825 -72.317825 -72.317825 -72.317825   \n",
       "514 -46.456718 -47.024212 -46.142891 -46.262913 -46.480347 -46.620770   \n",
       "\n",
       "           6          7          8          9    ...        207        208  \\\n",
       "317 -51.243423 -51.221672 -50.785576 -50.070919  ... -24.733873 -22.042908   \n",
       "877 -45.553383 -45.842388 -45.717918 -46.047836  ... -38.503208 -39.523758   \n",
       "277 -51.556725 -52.577854 -53.201706 -52.130268  ... -53.811703 -54.319889   \n",
       "304 -40.655987 -40.645638 -40.632103 -40.652580  ... -37.957699 -37.451443   \n",
       "102 -53.668484 -53.156345 -54.001930 -55.969082  ... -52.854710 -49.178703   \n",
       "367 -40.949989 -40.641487 -41.340652 -42.487789  ... -45.526016 -47.148186   \n",
       "741 -39.716457 -40.758728 -40.758728 -40.758728  ... -40.758728 -40.758728   \n",
       "279 -49.240765 -49.972069 -51.259541 -50.161118  ... -50.108318 -50.828201   \n",
       "460 -72.317825 -72.317825 -72.317825 -72.317825  ... -64.586044 -64.523933   \n",
       "514 -47.260395 -46.667107 -47.757446 -46.490688  ... -45.584679 -43.598026   \n",
       "\n",
       "           209        210        211        212        213        214  \\\n",
       "317 -21.389210 -22.735104 -21.326191 -19.974070 -19.913639 -18.530849   \n",
       "877 -40.019905 -41.528805 -42.962498 -42.645603 -42.775723 -42.864445   \n",
       "277 -55.268959 -52.052856 -50.433273 -53.498554 -53.312832 -52.798386   \n",
       "304 -39.119526 -39.735306 -40.298763 -40.656857 -40.656857 -40.656857   \n",
       "102 -49.217075 -50.817707 -50.598225 -50.945152 -50.485638 -51.919376   \n",
       "367 -46.474819 -44.839092 -46.956478 -47.138290 -48.784267 -46.908516   \n",
       "741 -40.758728 -40.758728 -40.758728 -40.758728 -40.758728 -40.758728   \n",
       "279 -51.525124 -50.562870 -49.983562 -52.057095 -50.964302 -50.562965   \n",
       "460 -64.174210 -66.311821 -63.141403 -60.602013 -63.572910 -64.221634   \n",
       "514 -46.936806 -47.840889 -48.035343 -47.782524 -45.367886 -45.400936   \n",
       "\n",
       "           215           0    \n",
       "317 -14.574443    male_happy  \n",
       "877 -42.943764  male_fearful  \n",
       "277 -51.479801    male_happy  \n",
       "304 -40.656857  female_happy  \n",
       "102 -52.982174   female_calm  \n",
       "367 -45.789280    male_happy  \n",
       "741 -40.758728    male_angry  \n",
       "279 -51.980816    male_happy  \n",
       "460 -63.324245    female_sad  \n",
       "514 -47.511478    female_sad  \n",
       "\n",
       "[10 rows x 217 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.utils import shuffle\n",
    "rnewdf = shuffle(newdf)\n",
    "rnewdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnewdf=rnewdf.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf1 = np.random.rand(len(rnewdf)) < 0.8\n",
    "train = rnewdf[newdf1]\n",
    "test = rnewdf[~newdf1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfeatures = train.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainlabel = train.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "testfeatures = test.iloc[:, :-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "testlabel = test.iloc[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from keras.utils import np_utils\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "X_train = np.array(trainfeatures)\n",
    "y_train = np.array(trainlabel)\n",
    "X_test = np.array(testfeatures)\n",
    "y_test = np.array(testlabel)\n",
    "\n",
    "lb = LabelEncoder()\n",
    "\n",
    "y_train = np_utils.to_categorical(lb.fit_transform(y_train))\n",
    "y_test = np_utils.to_categorical(lb.fit_transform(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(765, 216)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_traincnn =np.expand_dims(X_train, axis=2)\n",
    "x_testcnn= np.expand_dims(X_test, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv1D(256, 5,padding='same',\n",
    "                 input_shape=(216,1)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv1D(128, 5,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(MaxPooling1D(pool_size=(8)))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "#model.add(Conv1D(128, 5,padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Conv1D(128, 5,padding='same',))\n",
    "#model.add(Activation('relu'))\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Conv1D(128, 5,padding='same',))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "opt = keras.optimizers.rmsprop(lr=0.00001, decay=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_5 (Conv1D)            (None, 216, 256)          1536      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 216, 256)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 216, 128)          163968    \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 216, 128)          0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 27, 128)           82048     \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 27, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 3456)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                34570     \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 364,170\n",
      "Trainable params: 364,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0922 08:15:41.096383  9408 deprecation_wrapper.py:119] From C:\\Users\\lenovo\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 765 samples, validate on 194 samples\n",
      "Epoch 1/700\n",
      "765/765 [==============================] - 5s 6ms/step - loss: 2.9849 - accuracy: 0.0915 - val_loss: 2.3227 - val_accuracy: 0.1186\n",
      "Epoch 2/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 2.3100 - accuracy: 0.1190 - val_loss: 2.2661 - val_accuracy: 0.1598\n",
      "Epoch 3/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 2.2634 - accuracy: 0.1673 - val_loss: 2.2406 - val_accuracy: 0.1701\n",
      "Epoch 4/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 2.2376 - accuracy: 0.1712 - val_loss: 2.2194 - val_accuracy: 0.1649\n",
      "Epoch 5/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 2.2075 - accuracy: 0.1621 - val_loss: 2.2009 - val_accuracy: 0.2165\n",
      "Epoch 6/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 2.1785 - accuracy: 0.1961 - val_loss: 2.1631 - val_accuracy: 0.2268\n",
      "Epoch 7/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 2.1538 - accuracy: 0.2209 - val_loss: 2.1433 - val_accuracy: 0.1804\n",
      "Epoch 8/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 2.1336 - accuracy: 0.2065 - val_loss: 2.1184 - val_accuracy: 0.2629\n",
      "Epoch 9/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 2.1201 - accuracy: 0.2366 - val_loss: 2.1024 - val_accuracy: 0.2216\n",
      "Epoch 10/700\n",
      "765/765 [==============================] - 5s 6ms/step - loss: 2.1008 - accuracy: 0.2222 - val_loss: 2.0714 - val_accuracy: 0.3093\n",
      "Epoch 11/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 2.0861 - accuracy: 0.2484 - val_loss: 2.0685 - val_accuracy: 0.2371\n",
      "Epoch 12/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 2.0616 - accuracy: 0.2627 - val_loss: 2.0759 - val_accuracy: 0.2629\n",
      "Epoch 13/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 2.0462 - accuracy: 0.2627 - val_loss: 2.0316 - val_accuracy: 0.2680\n",
      "Epoch 14/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 2.0242 - accuracy: 0.2732 - val_loss: 2.0207 - val_accuracy: 0.3144\n",
      "Epoch 15/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 2.0194 - accuracy: 0.2549 - val_loss: 1.9943 - val_accuracy: 0.2784\n",
      "Epoch 16/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.9960 - accuracy: 0.2719 - val_loss: 1.9842 - val_accuracy: 0.2732\n",
      "Epoch 17/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.9810 - accuracy: 0.2876 - val_loss: 1.9666 - val_accuracy: 0.3402\n",
      "Epoch 18/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.9737 - accuracy: 0.2719 - val_loss: 1.9713 - val_accuracy: 0.3351\n",
      "Epoch 19/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.9569 - accuracy: 0.2863 - val_loss: 1.9398 - val_accuracy: 0.3144\n",
      "Epoch 20/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.9385 - accuracy: 0.2850 - val_loss: 1.9720 - val_accuracy: 0.2938\n",
      "Epoch 21/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.9325 - accuracy: 0.3137 - val_loss: 1.9434 - val_accuracy: 0.2990\n",
      "Epoch 22/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.9238 - accuracy: 0.3281 - val_loss: 1.9341 - val_accuracy: 0.3144\n",
      "Epoch 23/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.9070 - accuracy: 0.3320 - val_loss: 1.9378 - val_accuracy: 0.2938\n",
      "Epoch 24/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.8930 - accuracy: 0.3268 - val_loss: 1.9221 - val_accuracy: 0.2732\n",
      "Epoch 25/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.8846 - accuracy: 0.3281 - val_loss: 1.9021 - val_accuracy: 0.2835\n",
      "Epoch 26/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.8787 - accuracy: 0.3281 - val_loss: 1.8647 - val_accuracy: 0.3041\n",
      "Epoch 27/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.8664 - accuracy: 0.3320 - val_loss: 1.8676 - val_accuracy: 0.3299\n",
      "Epoch 28/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.8500 - accuracy: 0.3359 - val_loss: 1.8637 - val_accuracy: 0.3093\n",
      "Epoch 29/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.8438 - accuracy: 0.3373 - val_loss: 1.8560 - val_accuracy: 0.3454\n",
      "Epoch 30/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.8386 - accuracy: 0.3216 - val_loss: 1.8701 - val_accuracy: 0.3351\n",
      "Epoch 31/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.8228 - accuracy: 0.3425 - val_loss: 1.8360 - val_accuracy: 0.3299\n",
      "Epoch 32/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.8097 - accuracy: 0.3608 - val_loss: 1.8408 - val_accuracy: 0.2938\n",
      "Epoch 33/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.7987 - accuracy: 0.3582 - val_loss: 1.8481 - val_accuracy: 0.3196\n",
      "Epoch 34/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.7934 - accuracy: 0.3765 - val_loss: 1.8170 - val_accuracy: 0.3093\n",
      "Epoch 35/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.7805 - accuracy: 0.3699 - val_loss: 1.8113 - val_accuracy: 0.3402\n",
      "Epoch 36/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.7652 - accuracy: 0.3621 - val_loss: 1.8334 - val_accuracy: 0.3299\n",
      "Epoch 37/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.7649 - accuracy: 0.3595 - val_loss: 1.7826 - val_accuracy: 0.3196\n",
      "Epoch 38/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.7516 - accuracy: 0.3791 - val_loss: 1.7986 - val_accuracy: 0.3505\n",
      "Epoch 39/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.7501 - accuracy: 0.3647 - val_loss: 1.7688 - val_accuracy: 0.3711\n",
      "Epoch 40/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.7387 - accuracy: 0.3712 - val_loss: 1.7697 - val_accuracy: 0.3608\n",
      "Epoch 41/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.7216 - accuracy: 0.3869 - val_loss: 1.7439 - val_accuracy: 0.3608\n",
      "Epoch 42/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.7189 - accuracy: 0.3686 - val_loss: 1.7847 - val_accuracy: 0.3505\n",
      "Epoch 43/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.7107 - accuracy: 0.3725 - val_loss: 1.7709 - val_accuracy: 0.3505\n",
      "Epoch 44/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.7000 - accuracy: 0.3882 - val_loss: 1.7720 - val_accuracy: 0.3711\n",
      "Epoch 45/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6959 - accuracy: 0.3882 - val_loss: 1.7463 - val_accuracy: 0.3763\n",
      "Epoch 46/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6892 - accuracy: 0.4105 - val_loss: 1.7158 - val_accuracy: 0.4124\n",
      "Epoch 47/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6772 - accuracy: 0.4065 - val_loss: 1.7408 - val_accuracy: 0.3505\n",
      "Epoch 48/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6789 - accuracy: 0.3987 - val_loss: 1.6976 - val_accuracy: 0.3814\n",
      "Epoch 49/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6661 - accuracy: 0.4039 - val_loss: 1.7188 - val_accuracy: 0.3918\n",
      "Epoch 50/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6636 - accuracy: 0.3987 - val_loss: 1.6952 - val_accuracy: 0.3866\n",
      "Epoch 51/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6485 - accuracy: 0.4052 - val_loss: 1.6648 - val_accuracy: 0.3969\n",
      "Epoch 52/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6402 - accuracy: 0.4144 - val_loss: 1.6891 - val_accuracy: 0.3969\n",
      "Epoch 53/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6448 - accuracy: 0.3908 - val_loss: 1.6920 - val_accuracy: 0.3918\n",
      "Epoch 54/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6225 - accuracy: 0.4235 - val_loss: 1.7324 - val_accuracy: 0.3041\n",
      "Epoch 55/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6212 - accuracy: 0.4039 - val_loss: 1.6520 - val_accuracy: 0.3969\n",
      "Epoch 56/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6161 - accuracy: 0.4000 - val_loss: 1.6791 - val_accuracy: 0.3711\n",
      "Epoch 57/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/765 [==============================] - 4s 5ms/step - loss: 1.6026 - accuracy: 0.4196 - val_loss: 1.6635 - val_accuracy: 0.4021\n",
      "Epoch 58/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5869 - accuracy: 0.4183 - val_loss: 1.6542 - val_accuracy: 0.4021\n",
      "Epoch 59/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5923 - accuracy: 0.4118 - val_loss: 1.6372 - val_accuracy: 0.4433\n",
      "Epoch 60/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5896 - accuracy: 0.4092 - val_loss: 1.6493 - val_accuracy: 0.3918\n",
      "Epoch 61/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5791 - accuracy: 0.4078 - val_loss: 1.6372 - val_accuracy: 0.3969\n",
      "Epoch 62/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5651 - accuracy: 0.4170 - val_loss: 1.6208 - val_accuracy: 0.3918\n",
      "Epoch 63/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5540 - accuracy: 0.4327 - val_loss: 1.6963 - val_accuracy: 0.3711\n",
      "Epoch 64/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5577 - accuracy: 0.4405 - val_loss: 1.6350 - val_accuracy: 0.4021\n",
      "Epoch 65/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5446 - accuracy: 0.4235 - val_loss: 1.6692 - val_accuracy: 0.3454\n",
      "Epoch 66/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5464 - accuracy: 0.4379 - val_loss: 1.6011 - val_accuracy: 0.4072\n",
      "Epoch 67/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5378 - accuracy: 0.4458 - val_loss: 1.6303 - val_accuracy: 0.3969\n",
      "Epoch 68/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5329 - accuracy: 0.4458 - val_loss: 1.6175 - val_accuracy: 0.3866\n",
      "Epoch 69/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5207 - accuracy: 0.4588 - val_loss: 1.6012 - val_accuracy: 0.4021\n",
      "Epoch 70/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5270 - accuracy: 0.4392 - val_loss: 1.5641 - val_accuracy: 0.4278\n",
      "Epoch 71/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5047 - accuracy: 0.4562 - val_loss: 1.5806 - val_accuracy: 0.4072\n",
      "Epoch 72/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5076 - accuracy: 0.4418 - val_loss: 1.5688 - val_accuracy: 0.4278\n",
      "Epoch 73/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.5049 - accuracy: 0.4392 - val_loss: 1.5827 - val_accuracy: 0.4124\n",
      "Epoch 74/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4836 - accuracy: 0.4614 - val_loss: 1.5508 - val_accuracy: 0.4588\n",
      "Epoch 75/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4894 - accuracy: 0.4497 - val_loss: 1.5834 - val_accuracy: 0.4330\n",
      "Epoch 76/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4892 - accuracy: 0.4614 - val_loss: 1.5770 - val_accuracy: 0.4021\n",
      "Epoch 77/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4812 - accuracy: 0.4405 - val_loss: 1.5865 - val_accuracy: 0.4175\n",
      "Epoch 78/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4704 - accuracy: 0.4588 - val_loss: 1.5250 - val_accuracy: 0.4381\n",
      "Epoch 79/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4742 - accuracy: 0.4431 - val_loss: 1.5578 - val_accuracy: 0.4021\n",
      "Epoch 80/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4627 - accuracy: 0.4719 - val_loss: 1.5551 - val_accuracy: 0.4021\n",
      "Epoch 81/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4632 - accuracy: 0.4536 - val_loss: 1.5228 - val_accuracy: 0.4227\n",
      "Epoch 82/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4501 - accuracy: 0.4523 - val_loss: 1.5724 - val_accuracy: 0.4227\n",
      "Epoch 83/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4508 - accuracy: 0.4758 - val_loss: 1.5543 - val_accuracy: 0.4175\n",
      "Epoch 84/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4396 - accuracy: 0.4732 - val_loss: 1.5137 - val_accuracy: 0.4433\n",
      "Epoch 85/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4437 - accuracy: 0.4758 - val_loss: 1.5829 - val_accuracy: 0.3660\n",
      "Epoch 86/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 1.4344 - accuracy: 0.4601 - val_loss: 1.5233 - val_accuracy: 0.4330\n",
      "Epoch 87/700\n",
      "765/765 [==============================] - 5s 6ms/step - loss: 1.4394 - accuracy: 0.4536 - val_loss: 1.5100 - val_accuracy: 0.4278\n",
      "Epoch 88/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 1.4335 - accuracy: 0.4497 - val_loss: 1.5289 - val_accuracy: 0.4433\n",
      "Epoch 89/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 1.4257 - accuracy: 0.4562 - val_loss: 1.5027 - val_accuracy: 0.4175\n",
      "Epoch 90/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4245 - accuracy: 0.4797 - val_loss: 1.5081 - val_accuracy: 0.4485\n",
      "Epoch 91/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4211 - accuracy: 0.4667 - val_loss: 1.4950 - val_accuracy: 0.4691\n",
      "Epoch 92/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 1.4054 - accuracy: 0.4810 - val_loss: 1.4944 - val_accuracy: 0.4897\n",
      "Epoch 93/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.4125 - accuracy: 0.4745 - val_loss: 1.4708 - val_accuracy: 0.4639\n",
      "Epoch 94/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3951 - accuracy: 0.4876 - val_loss: 1.5867 - val_accuracy: 0.3814\n",
      "Epoch 95/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 1.4008 - accuracy: 0.4771 - val_loss: 1.4821 - val_accuracy: 0.4175\n",
      "Epoch 96/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 1.3831 - accuracy: 0.4797 - val_loss: 1.5259 - val_accuracy: 0.4278\n",
      "Epoch 97/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 1.3954 - accuracy: 0.4771 - val_loss: 1.5374 - val_accuracy: 0.4124\n",
      "Epoch 98/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 1.3868 - accuracy: 0.4967 - val_loss: 1.4600 - val_accuracy: 0.4845\n",
      "Epoch 99/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3798 - accuracy: 0.4745 - val_loss: 1.4906 - val_accuracy: 0.4330\n",
      "Epoch 100/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3722 - accuracy: 0.4889 - val_loss: 1.4641 - val_accuracy: 0.4691\n",
      "Epoch 101/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 1.3820 - accuracy: 0.4850 - val_loss: 1.4730 - val_accuracy: 0.4691\n",
      "Epoch 102/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 1.3754 - accuracy: 0.4889 - val_loss: 1.4720 - val_accuracy: 0.4433\n",
      "Epoch 103/700\n",
      "765/765 [==============================] - 5s 6ms/step - loss: 1.3796 - accuracy: 0.4797 - val_loss: 1.4567 - val_accuracy: 0.4794\n",
      "Epoch 104/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3654 - accuracy: 0.4876 - val_loss: 1.4678 - val_accuracy: 0.4536\n",
      "Epoch 105/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3647 - accuracy: 0.4954 - val_loss: 1.4634 - val_accuracy: 0.4330\n",
      "Epoch 106/700\n",
      "765/765 [==============================] - 5s 6ms/step - loss: 1.3615 - accuracy: 0.4954 - val_loss: 1.5028 - val_accuracy: 0.4381\n",
      "Epoch 107/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 1.3527 - accuracy: 0.5020 - val_loss: 1.4541 - val_accuracy: 0.4485\n",
      "Epoch 108/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3604 - accuracy: 0.4915 - val_loss: 1.4509 - val_accuracy: 0.4845\n",
      "Epoch 109/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3359 - accuracy: 0.5124 - val_loss: 1.4661 - val_accuracy: 0.4691\n",
      "Epoch 110/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3435 - accuracy: 0.5111 - val_loss: 1.4607 - val_accuracy: 0.4278\n",
      "Epoch 111/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3422 - accuracy: 0.5085 - val_loss: 1.4388 - val_accuracy: 0.4588\n",
      "Epoch 112/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3336 - accuracy: 0.5046 - val_loss: 1.4350 - val_accuracy: 0.4639\n",
      "Epoch 113/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3356 - accuracy: 0.5020 - val_loss: 1.4245 - val_accuracy: 0.4742\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 114/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3224 - accuracy: 0.5190 - val_loss: 1.4545 - val_accuracy: 0.4485\n",
      "Epoch 115/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3256 - accuracy: 0.5046 - val_loss: 1.4261 - val_accuracy: 0.4536\n",
      "Epoch 116/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3260 - accuracy: 0.5163 - val_loss: 1.4219 - val_accuracy: 0.4691\n",
      "Epoch 117/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3255 - accuracy: 0.4954 - val_loss: 1.4331 - val_accuracy: 0.4897\n",
      "Epoch 118/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3232 - accuracy: 0.5007 - val_loss: 1.4428 - val_accuracy: 0.4381\n",
      "Epoch 119/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3114 - accuracy: 0.4993 - val_loss: 1.4241 - val_accuracy: 0.5103\n",
      "Epoch 120/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3105 - accuracy: 0.5046 - val_loss: 1.3956 - val_accuracy: 0.5000\n",
      "Epoch 121/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3183 - accuracy: 0.5098 - val_loss: 1.4445 - val_accuracy: 0.4433\n",
      "Epoch 122/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3107 - accuracy: 0.5203 - val_loss: 1.4006 - val_accuracy: 0.4742\n",
      "Epoch 123/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3127 - accuracy: 0.5033 - val_loss: 1.4448 - val_accuracy: 0.4588\n",
      "Epoch 124/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2961 - accuracy: 0.4980 - val_loss: 1.4385 - val_accuracy: 0.4330\n",
      "Epoch 125/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3036 - accuracy: 0.5072 - val_loss: 1.3954 - val_accuracy: 0.4897\n",
      "Epoch 126/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.3020 - accuracy: 0.4915 - val_loss: 1.3866 - val_accuracy: 0.4588\n",
      "Epoch 127/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2955 - accuracy: 0.5163 - val_loss: 1.4117 - val_accuracy: 0.4742\n",
      "Epoch 128/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2930 - accuracy: 0.5294 - val_loss: 1.4702 - val_accuracy: 0.4124\n",
      "Epoch 129/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2932 - accuracy: 0.5098 - val_loss: 1.3887 - val_accuracy: 0.4897\n",
      "Epoch 130/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2887 - accuracy: 0.5137 - val_loss: 1.4223 - val_accuracy: 0.4536\n",
      "Epoch 131/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2911 - accuracy: 0.5059 - val_loss: 1.3764 - val_accuracy: 0.4742\n",
      "Epoch 132/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2772 - accuracy: 0.5190 - val_loss: 1.4993 - val_accuracy: 0.4278\n",
      "Epoch 133/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2764 - accuracy: 0.5255 - val_loss: 1.4143 - val_accuracy: 0.4742\n",
      "Epoch 134/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2804 - accuracy: 0.5124 - val_loss: 1.3797 - val_accuracy: 0.4948\n",
      "Epoch 135/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2668 - accuracy: 0.5438 - val_loss: 1.4071 - val_accuracy: 0.4639\n",
      "Epoch 136/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2731 - accuracy: 0.5229 - val_loss: 1.4185 - val_accuracy: 0.4588\n",
      "Epoch 137/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2686 - accuracy: 0.5150 - val_loss: 1.4276 - val_accuracy: 0.4588\n",
      "Epoch 138/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2711 - accuracy: 0.5203 - val_loss: 1.4037 - val_accuracy: 0.5052\n",
      "Epoch 139/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2734 - accuracy: 0.5137 - val_loss: 1.4048 - val_accuracy: 0.4948\n",
      "Epoch 140/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2646 - accuracy: 0.5163 - val_loss: 1.4218 - val_accuracy: 0.4742\n",
      "Epoch 141/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2717 - accuracy: 0.5176 - val_loss: 1.3722 - val_accuracy: 0.5052\n",
      "Epoch 142/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2513 - accuracy: 0.5399 - val_loss: 1.3743 - val_accuracy: 0.4588\n",
      "Epoch 143/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2546 - accuracy: 0.5229 - val_loss: 1.4059 - val_accuracy: 0.4691\n",
      "Epoch 144/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2537 - accuracy: 0.5373 - val_loss: 1.4001 - val_accuracy: 0.4639\n",
      "Epoch 145/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2460 - accuracy: 0.5438 - val_loss: 1.3790 - val_accuracy: 0.5000\n",
      "Epoch 146/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2411 - accuracy: 0.5359 - val_loss: 1.4020 - val_accuracy: 0.4485\n",
      "Epoch 147/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2350 - accuracy: 0.5490 - val_loss: 1.3637 - val_accuracy: 0.4691\n",
      "Epoch 148/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2474 - accuracy: 0.5255 - val_loss: 1.3869 - val_accuracy: 0.5103\n",
      "Epoch 149/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2431 - accuracy: 0.5255 - val_loss: 1.3577 - val_accuracy: 0.4897\n",
      "Epoch 150/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2451 - accuracy: 0.5333 - val_loss: 1.3485 - val_accuracy: 0.5000\n",
      "Epoch 151/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2354 - accuracy: 0.5556 - val_loss: 1.3756 - val_accuracy: 0.4639\n",
      "Epoch 152/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2393 - accuracy: 0.5438 - val_loss: 1.3716 - val_accuracy: 0.5000\n",
      "Epoch 153/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2262 - accuracy: 0.5307 - val_loss: 1.5189 - val_accuracy: 0.3814\n",
      "Epoch 154/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2332 - accuracy: 0.5346 - val_loss: 1.3919 - val_accuracy: 0.5000\n",
      "Epoch 155/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2319 - accuracy: 0.5307 - val_loss: 1.3845 - val_accuracy: 0.4948\n",
      "Epoch 156/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2243 - accuracy: 0.5373 - val_loss: 1.4203 - val_accuracy: 0.4588\n",
      "Epoch 157/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2263 - accuracy: 0.5229 - val_loss: 1.3663 - val_accuracy: 0.5309\n",
      "Epoch 158/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2253 - accuracy: 0.5229 - val_loss: 1.4079 - val_accuracy: 0.4897\n",
      "Epoch 159/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2217 - accuracy: 0.5399 - val_loss: 1.3960 - val_accuracy: 0.4948\n",
      "Epoch 160/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2147 - accuracy: 0.5490 - val_loss: 1.3518 - val_accuracy: 0.5103\n",
      "Epoch 161/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2150 - accuracy: 0.5438 - val_loss: 1.3509 - val_accuracy: 0.5258\n",
      "Epoch 162/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2096 - accuracy: 0.5529 - val_loss: 1.4358 - val_accuracy: 0.4227\n",
      "Epoch 163/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2272 - accuracy: 0.5503 - val_loss: 1.3525 - val_accuracy: 0.5155\n",
      "Epoch 164/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2106 - accuracy: 0.5333 - val_loss: 1.3769 - val_accuracy: 0.4845\n",
      "Epoch 165/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2103 - accuracy: 0.5399 - val_loss: 1.3481 - val_accuracy: 0.5258\n",
      "Epoch 166/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2009 - accuracy: 0.5425 - val_loss: 1.3398 - val_accuracy: 0.4588\n",
      "Epoch 167/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1981 - accuracy: 0.5438 - val_loss: 1.3756 - val_accuracy: 0.4742\n",
      "Epoch 168/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.2015 - accuracy: 0.5673 - val_loss: 1.3908 - val_accuracy: 0.4691\n",
      "Epoch 169/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1937 - accuracy: 0.5438 - val_loss: 1.4264 - val_accuracy: 0.4691\n",
      "Epoch 170/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1864 - accuracy: 0.5516 - val_loss: 1.3825 - val_accuracy: 0.5000\n",
      "Epoch 171/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1918 - accuracy: 0.5333 - val_loss: 1.4155 - val_accuracy: 0.4433\n",
      "Epoch 172/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1821 - accuracy: 0.5516 - val_loss: 1.3217 - val_accuracy: 0.5052\n",
      "Epoch 173/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1861 - accuracy: 0.5608 - val_loss: 1.3411 - val_accuracy: 0.4691\n",
      "Epoch 174/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1861 - accuracy: 0.5647 - val_loss: 1.4284 - val_accuracy: 0.4845\n",
      "Epoch 175/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1794 - accuracy: 0.5412 - val_loss: 1.3506 - val_accuracy: 0.5258\n",
      "Epoch 176/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1729 - accuracy: 0.5660 - val_loss: 1.3570 - val_accuracy: 0.5000\n",
      "Epoch 177/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1761 - accuracy: 0.5529 - val_loss: 1.3391 - val_accuracy: 0.5103\n",
      "Epoch 178/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1677 - accuracy: 0.5582 - val_loss: 1.3476 - val_accuracy: 0.4794\n",
      "Epoch 179/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1771 - accuracy: 0.5621 - val_loss: 1.3648 - val_accuracy: 0.4639\n",
      "Epoch 180/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1808 - accuracy: 0.5516 - val_loss: 1.3496 - val_accuracy: 0.5258\n",
      "Epoch 181/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1730 - accuracy: 0.5477 - val_loss: 1.3504 - val_accuracy: 0.4588\n",
      "Epoch 182/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1556 - accuracy: 0.5660 - val_loss: 1.4487 - val_accuracy: 0.4278\n",
      "Epoch 183/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1666 - accuracy: 0.5582 - val_loss: 1.3537 - val_accuracy: 0.5258\n",
      "Epoch 184/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1683 - accuracy: 0.5621 - val_loss: 1.3522 - val_accuracy: 0.5052\n",
      "Epoch 185/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1599 - accuracy: 0.5634 - val_loss: 1.3317 - val_accuracy: 0.5052\n",
      "Epoch 186/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1629 - accuracy: 0.5686 - val_loss: 1.3217 - val_accuracy: 0.5309\n",
      "Epoch 187/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1513 - accuracy: 0.5712 - val_loss: 1.3318 - val_accuracy: 0.5155\n",
      "Epoch 188/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1624 - accuracy: 0.5634 - val_loss: 1.3485 - val_accuracy: 0.4742\n",
      "Epoch 189/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1459 - accuracy: 0.5948 - val_loss: 1.3279 - val_accuracy: 0.5361\n",
      "Epoch 190/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1365 - accuracy: 0.5778 - val_loss: 1.3429 - val_accuracy: 0.5052\n",
      "Epoch 191/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1503 - accuracy: 0.5686 - val_loss: 1.3096 - val_accuracy: 0.5361\n",
      "Epoch 192/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1399 - accuracy: 0.5647 - val_loss: 1.3296 - val_accuracy: 0.5052\n",
      "Epoch 193/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1408 - accuracy: 0.5791 - val_loss: 1.3367 - val_accuracy: 0.5052\n",
      "Epoch 194/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1468 - accuracy: 0.5935 - val_loss: 1.3175 - val_accuracy: 0.5103\n",
      "Epoch 195/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1533 - accuracy: 0.5739 - val_loss: 1.3210 - val_accuracy: 0.5103\n",
      "Epoch 196/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1258 - accuracy: 0.5778 - val_loss: 1.3905 - val_accuracy: 0.4845\n",
      "Epoch 197/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1415 - accuracy: 0.5843 - val_loss: 1.4598 - val_accuracy: 0.4124\n",
      "Epoch 198/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1281 - accuracy: 0.6105 - val_loss: 1.3897 - val_accuracy: 0.5000\n",
      "Epoch 199/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1374 - accuracy: 0.5804 - val_loss: 1.3614 - val_accuracy: 0.5155\n",
      "Epoch 200/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1228 - accuracy: 0.6013 - val_loss: 1.3203 - val_accuracy: 0.4845\n",
      "Epoch 201/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1309 - accuracy: 0.5882 - val_loss: 1.3380 - val_accuracy: 0.5000\n",
      "Epoch 202/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1258 - accuracy: 0.6000 - val_loss: 1.3831 - val_accuracy: 0.4948\n",
      "Epoch 203/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1224 - accuracy: 0.5895 - val_loss: 1.3717 - val_accuracy: 0.4948\n",
      "Epoch 204/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1295 - accuracy: 0.5922 - val_loss: 1.3403 - val_accuracy: 0.4897\n",
      "Epoch 205/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1122 - accuracy: 0.6039 - val_loss: 1.3584 - val_accuracy: 0.5206\n",
      "Epoch 206/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1206 - accuracy: 0.5961 - val_loss: 1.3663 - val_accuracy: 0.4639\n",
      "Epoch 207/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1163 - accuracy: 0.5856 - val_loss: 1.3813 - val_accuracy: 0.4948\n",
      "Epoch 208/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1074 - accuracy: 0.5895 - val_loss: 1.3647 - val_accuracy: 0.4794\n",
      "Epoch 209/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1136 - accuracy: 0.5778 - val_loss: 1.3258 - val_accuracy: 0.4794\n",
      "Epoch 210/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1075 - accuracy: 0.5987 - val_loss: 1.3002 - val_accuracy: 0.5258\n",
      "Epoch 211/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1011 - accuracy: 0.6065 - val_loss: 1.3233 - val_accuracy: 0.5412\n",
      "Epoch 212/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0999 - accuracy: 0.6144 - val_loss: 1.3452 - val_accuracy: 0.5206\n",
      "Epoch 213/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1069 - accuracy: 0.5987 - val_loss: 1.3368 - val_accuracy: 0.5155\n",
      "Epoch 214/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1055 - accuracy: 0.5948 - val_loss: 1.3311 - val_accuracy: 0.5309\n",
      "Epoch 215/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0963 - accuracy: 0.6092 - val_loss: 1.3790 - val_accuracy: 0.4536\n",
      "Epoch 216/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 1.1044 - accuracy: 0.5948 - val_loss: 1.3222 - val_accuracy: 0.5103\n",
      "Epoch 217/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.1013 - accuracy: 0.5974 - val_loss: 1.3161 - val_accuracy: 0.4948\n",
      "Epoch 218/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0956 - accuracy: 0.5908 - val_loss: 1.3337 - val_accuracy: 0.5103\n",
      "Epoch 219/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0905 - accuracy: 0.6118 - val_loss: 1.3249 - val_accuracy: 0.4897\n",
      "Epoch 220/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0901 - accuracy: 0.6183 - val_loss: 1.3237 - val_accuracy: 0.4794\n",
      "Epoch 221/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0907 - accuracy: 0.5961 - val_loss: 1.2809 - val_accuracy: 0.5361\n",
      "Epoch 222/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0805 - accuracy: 0.6131 - val_loss: 1.3237 - val_accuracy: 0.5206\n",
      "Epoch 223/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0857 - accuracy: 0.5935 - val_loss: 1.3983 - val_accuracy: 0.4485\n",
      "Epoch 224/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0772 - accuracy: 0.6013 - val_loss: 1.2818 - val_accuracy: 0.5103\n",
      "Epoch 225/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0763 - accuracy: 0.6261 - val_loss: 1.3750 - val_accuracy: 0.5155\n",
      "Epoch 226/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0862 - accuracy: 0.6000 - val_loss: 1.3046 - val_accuracy: 0.5155\n",
      "Epoch 227/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0694 - accuracy: 0.6222 - val_loss: 1.3482 - val_accuracy: 0.5052\n",
      "Epoch 228/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0588 - accuracy: 0.6131 - val_loss: 1.3194 - val_accuracy: 0.5155\n",
      "Epoch 229/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0783 - accuracy: 0.5948 - val_loss: 1.3323 - val_accuracy: 0.5258\n",
      "Epoch 230/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0748 - accuracy: 0.6131 - val_loss: 1.3362 - val_accuracy: 0.4897\n",
      "Epoch 231/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0571 - accuracy: 0.6314 - val_loss: 1.3332 - val_accuracy: 0.5155\n",
      "Epoch 232/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0613 - accuracy: 0.6052 - val_loss: 1.3119 - val_accuracy: 0.4897\n",
      "Epoch 233/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0619 - accuracy: 0.6157 - val_loss: 1.3207 - val_accuracy: 0.5155\n",
      "Epoch 234/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0600 - accuracy: 0.6078 - val_loss: 1.3465 - val_accuracy: 0.5258\n",
      "Epoch 235/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0563 - accuracy: 0.6261 - val_loss: 1.3483 - val_accuracy: 0.4742\n",
      "Epoch 236/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0621 - accuracy: 0.6196 - val_loss: 1.3036 - val_accuracy: 0.5361\n",
      "Epoch 237/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0581 - accuracy: 0.6065 - val_loss: 1.3228 - val_accuracy: 0.5052\n",
      "Epoch 238/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0530 - accuracy: 0.6170 - val_loss: 1.3176 - val_accuracy: 0.4794\n",
      "Epoch 239/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0533 - accuracy: 0.6209 - val_loss: 1.3278 - val_accuracy: 0.4897\n",
      "Epoch 240/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0539 - accuracy: 0.6196 - val_loss: 1.3264 - val_accuracy: 0.5206\n",
      "Epoch 241/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0527 - accuracy: 0.6209 - val_loss: 1.3517 - val_accuracy: 0.4897\n",
      "Epoch 242/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0519 - accuracy: 0.6052 - val_loss: 1.3753 - val_accuracy: 0.4485\n",
      "Epoch 243/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0324 - accuracy: 0.6248 - val_loss: 1.3110 - val_accuracy: 0.5000\n",
      "Epoch 244/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0492 - accuracy: 0.6340 - val_loss: 1.3072 - val_accuracy: 0.5206\n",
      "Epoch 245/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0382 - accuracy: 0.6248 - val_loss: 1.3763 - val_accuracy: 0.4897\n",
      "Epoch 246/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0489 - accuracy: 0.6275 - val_loss: 1.2956 - val_accuracy: 0.4897\n",
      "Epoch 247/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0210 - accuracy: 0.6288 - val_loss: 1.2901 - val_accuracy: 0.5258\n",
      "Epoch 248/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0361 - accuracy: 0.6157 - val_loss: 1.3414 - val_accuracy: 0.4845\n",
      "Epoch 249/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0384 - accuracy: 0.6340 - val_loss: 1.3400 - val_accuracy: 0.4845\n",
      "Epoch 250/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0309 - accuracy: 0.6261 - val_loss: 1.3247 - val_accuracy: 0.4897\n",
      "Epoch 251/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0242 - accuracy: 0.6405 - val_loss: 1.3310 - val_accuracy: 0.5103\n",
      "Epoch 252/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0349 - accuracy: 0.6157 - val_loss: 1.3114 - val_accuracy: 0.5309\n",
      "Epoch 253/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0267 - accuracy: 0.6261 - val_loss: 1.2916 - val_accuracy: 0.4948\n",
      "Epoch 254/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 1.0256 - accuracy: 0.6301 - val_loss: 1.3168 - val_accuracy: 0.4845\n",
      "Epoch 255/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 1.0267 - accuracy: 0.6248 - val_loss: 1.2955 - val_accuracy: 0.5103\n",
      "Epoch 256/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 1.0200 - accuracy: 0.6288 - val_loss: 1.3098 - val_accuracy: 0.5206\n",
      "Epoch 257/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 1.0189 - accuracy: 0.6405 - val_loss: 1.3283 - val_accuracy: 0.4948\n",
      "Epoch 258/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 1.0125 - accuracy: 0.6458 - val_loss: 1.3343 - val_accuracy: 0.4691\n",
      "Epoch 259/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0106 - accuracy: 0.6288 - val_loss: 1.2961 - val_accuracy: 0.5000\n",
      "Epoch 260/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 1.0175 - accuracy: 0.6248 - val_loss: 1.2943 - val_accuracy: 0.5155\n",
      "Epoch 261/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 1.0177 - accuracy: 0.6392 - val_loss: 1.3129 - val_accuracy: 0.5052\n",
      "Epoch 262/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 1.0129 - accuracy: 0.6353 - val_loss: 1.3423 - val_accuracy: 0.4948\n",
      "Epoch 263/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 1.0097 - accuracy: 0.6261 - val_loss: 1.3263 - val_accuracy: 0.4897\n",
      "Epoch 264/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9953 - accuracy: 0.6484 - val_loss: 1.3172 - val_accuracy: 0.4845\n",
      "Epoch 265/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9945 - accuracy: 0.6471 - val_loss: 1.3445 - val_accuracy: 0.5000\n",
      "Epoch 266/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 1.0019 - accuracy: 0.6431 - val_loss: 1.3303 - val_accuracy: 0.4639\n",
      "Epoch 267/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 1.0068 - accuracy: 0.6471 - val_loss: 1.3222 - val_accuracy: 0.4845\n",
      "Epoch 268/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9976 - accuracy: 0.6444 - val_loss: 1.3106 - val_accuracy: 0.4948\n",
      "Epoch 269/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9959 - accuracy: 0.6458 - val_loss: 1.3265 - val_accuracy: 0.4845\n",
      "Epoch 270/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9917 - accuracy: 0.6523 - val_loss: 1.3105 - val_accuracy: 0.5206\n",
      "Epoch 271/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9978 - accuracy: 0.6392 - val_loss: 1.3057 - val_accuracy: 0.5103\n",
      "Epoch 272/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9866 - accuracy: 0.6693 - val_loss: 1.3022 - val_accuracy: 0.4845\n",
      "Epoch 273/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9905 - accuracy: 0.6418 - val_loss: 1.3189 - val_accuracy: 0.5258\n",
      "Epoch 274/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9865 - accuracy: 0.6366 - val_loss: 1.4342 - val_accuracy: 0.4330\n",
      "Epoch 275/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9903 - accuracy: 0.6327 - val_loss: 1.3098 - val_accuracy: 0.5052\n",
      "Epoch 276/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9854 - accuracy: 0.6641 - val_loss: 1.3818 - val_accuracy: 0.4639\n",
      "Epoch 277/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9800 - accuracy: 0.6601 - val_loss: 1.3555 - val_accuracy: 0.4536\n",
      "Epoch 278/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9760 - accuracy: 0.6379 - val_loss: 1.3027 - val_accuracy: 0.5052\n",
      "Epoch 279/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9634 - accuracy: 0.6693 - val_loss: 1.3415 - val_accuracy: 0.4691\n",
      "Epoch 280/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9748 - accuracy: 0.6510 - val_loss: 1.3061 - val_accuracy: 0.5103\n",
      "Epoch 281/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9710 - accuracy: 0.6575 - val_loss: 1.3134 - val_accuracy: 0.5206\n",
      "Epoch 282/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9669 - accuracy: 0.6549 - val_loss: 1.3304 - val_accuracy: 0.4742\n",
      "Epoch 283/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9683 - accuracy: 0.6562 - val_loss: 1.3376 - val_accuracy: 0.5258\n",
      "Epoch 284/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9661 - accuracy: 0.6641 - val_loss: 1.3247 - val_accuracy: 0.5103\n",
      "Epoch 285/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9701 - accuracy: 0.6418 - val_loss: 1.3062 - val_accuracy: 0.5000\n",
      "Epoch 286/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9642 - accuracy: 0.6562 - val_loss: 1.3064 - val_accuracy: 0.4948\n",
      "Epoch 287/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9626 - accuracy: 0.6654 - val_loss: 1.3339 - val_accuracy: 0.4845\n",
      "Epoch 288/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9664 - accuracy: 0.6458 - val_loss: 1.2758 - val_accuracy: 0.5103\n",
      "Epoch 289/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9599 - accuracy: 0.6667 - val_loss: 1.3302 - val_accuracy: 0.5000\n",
      "Epoch 290/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9564 - accuracy: 0.6588 - val_loss: 1.3149 - val_accuracy: 0.5103\n",
      "Epoch 291/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9627 - accuracy: 0.6641 - val_loss: 1.3639 - val_accuracy: 0.4845\n",
      "Epoch 292/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9484 - accuracy: 0.6654 - val_loss: 1.3252 - val_accuracy: 0.5052\n",
      "Epoch 293/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9474 - accuracy: 0.6536 - val_loss: 1.3079 - val_accuracy: 0.4794\n",
      "Epoch 294/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9460 - accuracy: 0.6837 - val_loss: 1.3058 - val_accuracy: 0.4897\n",
      "Epoch 295/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9463 - accuracy: 0.6719 - val_loss: 1.3142 - val_accuracy: 0.5103\n",
      "Epoch 296/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9464 - accuracy: 0.6680 - val_loss: 1.3431 - val_accuracy: 0.4845\n",
      "Epoch 297/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9468 - accuracy: 0.6667 - val_loss: 1.3563 - val_accuracy: 0.5052\n",
      "Epoch 298/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9409 - accuracy: 0.6732 - val_loss: 1.3374 - val_accuracy: 0.4845\n",
      "Epoch 299/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9454 - accuracy: 0.6745 - val_loss: 1.2976 - val_accuracy: 0.5103\n",
      "Epoch 300/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9325 - accuracy: 0.6732 - val_loss: 1.3520 - val_accuracy: 0.4742\n",
      "Epoch 301/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9424 - accuracy: 0.6784 - val_loss: 1.3476 - val_accuracy: 0.4948\n",
      "Epoch 302/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9293 - accuracy: 0.6758 - val_loss: 1.3036 - val_accuracy: 0.5052\n",
      "Epoch 303/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9326 - accuracy: 0.6601 - val_loss: 1.2888 - val_accuracy: 0.5155\n",
      "Epoch 304/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9216 - accuracy: 0.6680 - val_loss: 1.2924 - val_accuracy: 0.5052\n",
      "Epoch 305/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9247 - accuracy: 0.6797 - val_loss: 1.3616 - val_accuracy: 0.4485\n",
      "Epoch 306/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9267 - accuracy: 0.6797 - val_loss: 1.3574 - val_accuracy: 0.4897\n",
      "Epoch 307/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9299 - accuracy: 0.6732 - val_loss: 1.2876 - val_accuracy: 0.5000\n",
      "Epoch 308/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9210 - accuracy: 0.6810 - val_loss: 1.3445 - val_accuracy: 0.4742\n",
      "Epoch 309/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9159 - accuracy: 0.6745 - val_loss: 1.3668 - val_accuracy: 0.5000\n",
      "Epoch 310/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9205 - accuracy: 0.6797 - val_loss: 1.4324 - val_accuracy: 0.4691\n",
      "Epoch 311/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9138 - accuracy: 0.6824 - val_loss: 1.4663 - val_accuracy: 0.4175\n",
      "Epoch 312/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9080 - accuracy: 0.6771 - val_loss: 1.3588 - val_accuracy: 0.4381\n",
      "Epoch 313/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9131 - accuracy: 0.6797 - val_loss: 1.3564 - val_accuracy: 0.5155\n",
      "Epoch 314/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9041 - accuracy: 0.6941 - val_loss: 1.2951 - val_accuracy: 0.5103\n",
      "Epoch 315/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.9057 - accuracy: 0.6719 - val_loss: 1.3467 - val_accuracy: 0.4948\n",
      "Epoch 316/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9024 - accuracy: 0.6863 - val_loss: 1.3111 - val_accuracy: 0.5000\n",
      "Epoch 317/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.9024 - accuracy: 0.6889 - val_loss: 1.3260 - val_accuracy: 0.4639\n",
      "Epoch 318/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8997 - accuracy: 0.6719 - val_loss: 1.2899 - val_accuracy: 0.5155\n",
      "Epoch 319/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9022 - accuracy: 0.6993 - val_loss: 1.3038 - val_accuracy: 0.4948\n",
      "Epoch 320/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8962 - accuracy: 0.7007 - val_loss: 1.2943 - val_accuracy: 0.5155\n",
      "Epoch 321/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9032 - accuracy: 0.6863 - val_loss: 1.3262 - val_accuracy: 0.5052\n",
      "Epoch 322/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8898 - accuracy: 0.6771 - val_loss: 1.3220 - val_accuracy: 0.4948\n",
      "Epoch 323/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.9012 - accuracy: 0.6980 - val_loss: 1.2952 - val_accuracy: 0.5309\n",
      "Epoch 324/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8955 - accuracy: 0.6680 - val_loss: 1.3236 - val_accuracy: 0.4897\n",
      "Epoch 325/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8943 - accuracy: 0.6837 - val_loss: 1.3159 - val_accuracy: 0.4948\n",
      "Epoch 326/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8879 - accuracy: 0.7007 - val_loss: 1.3769 - val_accuracy: 0.4742\n",
      "Epoch 327/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8899 - accuracy: 0.6954 - val_loss: 1.3300 - val_accuracy: 0.4845\n",
      "Epoch 328/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8767 - accuracy: 0.6954 - val_loss: 1.3029 - val_accuracy: 0.5155\n",
      "Epoch 329/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.8936 - accuracy: 0.6928 - val_loss: 1.3118 - val_accuracy: 0.5361\n",
      "Epoch 330/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.8838 - accuracy: 0.6928 - val_loss: 1.3012 - val_accuracy: 0.5258\n",
      "Epoch 331/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8751 - accuracy: 0.6928 - val_loss: 1.3071 - val_accuracy: 0.5103\n",
      "Epoch 332/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8866 - accuracy: 0.6993 - val_loss: 1.3060 - val_accuracy: 0.5000\n",
      "Epoch 333/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8741 - accuracy: 0.6993 - val_loss: 1.2752 - val_accuracy: 0.5309\n",
      "Epoch 334/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8795 - accuracy: 0.6941 - val_loss: 1.3488 - val_accuracy: 0.5000\n",
      "Epoch 335/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8680 - accuracy: 0.7007 - val_loss: 1.3338 - val_accuracy: 0.4948\n",
      "Epoch 336/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8728 - accuracy: 0.6980 - val_loss: 1.2967 - val_accuracy: 0.5052\n",
      "Epoch 337/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8658 - accuracy: 0.6993 - val_loss: 1.3027 - val_accuracy: 0.5206\n",
      "Epoch 338/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8774 - accuracy: 0.7007 - val_loss: 1.3082 - val_accuracy: 0.5052\n",
      "Epoch 339/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.8558 - accuracy: 0.7085 - val_loss: 1.2906 - val_accuracy: 0.4897\n",
      "Epoch 340/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8593 - accuracy: 0.7150 - val_loss: 1.3906 - val_accuracy: 0.4588\n",
      "Epoch 341/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.8737 - accuracy: 0.6941 - val_loss: 1.3752 - val_accuracy: 0.4639\n",
      "Epoch 342/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.8528 - accuracy: 0.7163 - val_loss: 1.3421 - val_accuracy: 0.4588\n",
      "Epoch 343/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.8559 - accuracy: 0.7072 - val_loss: 1.3884 - val_accuracy: 0.4897\n",
      "Epoch 344/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8561 - accuracy: 0.6876 - val_loss: 1.3655 - val_accuracy: 0.4742\n",
      "Epoch 345/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.8534 - accuracy: 0.7033 - val_loss: 1.3284 - val_accuracy: 0.4845\n",
      "Epoch 346/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8615 - accuracy: 0.7007 - val_loss: 1.3004 - val_accuracy: 0.4948\n",
      "Epoch 347/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8513 - accuracy: 0.7255 - val_loss: 1.3295 - val_accuracy: 0.5000\n",
      "Epoch 348/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.8534 - accuracy: 0.7059 - val_loss: 1.3147 - val_accuracy: 0.5052\n",
      "Epoch 349/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.8466 - accuracy: 0.6954 - val_loss: 1.3226 - val_accuracy: 0.4897\n",
      "Epoch 350/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8447 - accuracy: 0.7255 - val_loss: 1.3599 - val_accuracy: 0.4330\n",
      "Epoch 351/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.8428 - accuracy: 0.7137 - val_loss: 1.4011 - val_accuracy: 0.4536\n",
      "Epoch 352/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.8357 - accuracy: 0.7255 - val_loss: 1.3445 - val_accuracy: 0.4433\n",
      "Epoch 353/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.8370 - accuracy: 0.7085 - val_loss: 1.3361 - val_accuracy: 0.5155\n",
      "Epoch 354/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.8462 - accuracy: 0.7163 - val_loss: 1.2955 - val_accuracy: 0.5155\n",
      "Epoch 355/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8353 - accuracy: 0.7176 - val_loss: 1.3235 - val_accuracy: 0.5103\n",
      "Epoch 356/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.8320 - accuracy: 0.7085 - val_loss: 1.3212 - val_accuracy: 0.5155\n",
      "Epoch 357/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.8355 - accuracy: 0.7216 - val_loss: 1.3234 - val_accuracy: 0.4639\n",
      "Epoch 358/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.8189 - accuracy: 0.7085 - val_loss: 1.4242 - val_accuracy: 0.5052\n",
      "Epoch 359/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.8327 - accuracy: 0.7163 - val_loss: 1.3692 - val_accuracy: 0.4691\n",
      "Epoch 360/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8298 - accuracy: 0.7255 - val_loss: 1.2946 - val_accuracy: 0.5155\n",
      "Epoch 361/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8284 - accuracy: 0.7111 - val_loss: 1.4271 - val_accuracy: 0.4485\n",
      "Epoch 362/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8255 - accuracy: 0.7137 - val_loss: 1.3012 - val_accuracy: 0.5052\n",
      "Epoch 363/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8212 - accuracy: 0.7150 - val_loss: 1.3335 - val_accuracy: 0.5052\n",
      "Epoch 364/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8228 - accuracy: 0.7307 - val_loss: 1.3485 - val_accuracy: 0.4691\n",
      "Epoch 365/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8165 - accuracy: 0.7255 - val_loss: 1.2966 - val_accuracy: 0.5412\n",
      "Epoch 366/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8144 - accuracy: 0.7242 - val_loss: 1.3147 - val_accuracy: 0.5000\n",
      "Epoch 367/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8194 - accuracy: 0.7203 - val_loss: 1.3382 - val_accuracy: 0.4691\n",
      "Epoch 368/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8179 - accuracy: 0.7268 - val_loss: 1.2930 - val_accuracy: 0.5052\n",
      "Epoch 369/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.8103 - accuracy: 0.7320 - val_loss: 1.3171 - val_accuracy: 0.4794\n",
      "Epoch 370/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8079 - accuracy: 0.7399 - val_loss: 1.3324 - val_accuracy: 0.4845\n",
      "Epoch 371/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8098 - accuracy: 0.7242 - val_loss: 1.3770 - val_accuracy: 0.4742\n",
      "Epoch 372/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8092 - accuracy: 0.7176 - val_loss: 1.3156 - val_accuracy: 0.4897\n",
      "Epoch 373/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8025 - accuracy: 0.7281 - val_loss: 1.3958 - val_accuracy: 0.4639\n",
      "Epoch 374/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8015 - accuracy: 0.7229 - val_loss: 1.3550 - val_accuracy: 0.4588\n",
      "Epoch 375/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8048 - accuracy: 0.7399 - val_loss: 1.3041 - val_accuracy: 0.4794\n",
      "Epoch 376/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8098 - accuracy: 0.7229 - val_loss: 1.3208 - val_accuracy: 0.4948\n",
      "Epoch 377/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7889 - accuracy: 0.7542 - val_loss: 1.3792 - val_accuracy: 0.4691\n",
      "Epoch 378/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7961 - accuracy: 0.7373 - val_loss: 1.2856 - val_accuracy: 0.5000\n",
      "Epoch 379/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.8023 - accuracy: 0.7281 - val_loss: 1.3061 - val_accuracy: 0.5103\n",
      "Epoch 380/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7922 - accuracy: 0.7346 - val_loss: 1.3468 - val_accuracy: 0.5000\n",
      "Epoch 381/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7859 - accuracy: 0.7307 - val_loss: 1.3628 - val_accuracy: 0.5052\n",
      "Epoch 382/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7852 - accuracy: 0.7346 - val_loss: 1.3236 - val_accuracy: 0.5206\n",
      "Epoch 383/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7769 - accuracy: 0.7373 - val_loss: 1.3574 - val_accuracy: 0.4691\n",
      "Epoch 384/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7888 - accuracy: 0.7229 - val_loss: 1.2932 - val_accuracy: 0.5155\n",
      "Epoch 385/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7859 - accuracy: 0.7359 - val_loss: 1.3361 - val_accuracy: 0.4948\n",
      "Epoch 386/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7793 - accuracy: 0.7307 - val_loss: 1.4128 - val_accuracy: 0.4330\n",
      "Epoch 387/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7752 - accuracy: 0.7399 - val_loss: 1.3279 - val_accuracy: 0.4948\n",
      "Epoch 388/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7721 - accuracy: 0.7386 - val_loss: 1.2903 - val_accuracy: 0.5000\n",
      "Epoch 389/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7667 - accuracy: 0.7503 - val_loss: 1.3173 - val_accuracy: 0.4948\n",
      "Epoch 390/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7763 - accuracy: 0.7503 - val_loss: 1.4715 - val_accuracy: 0.4330\n",
      "Epoch 391/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7691 - accuracy: 0.7176 - val_loss: 1.3233 - val_accuracy: 0.5052\n",
      "Epoch 392/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7666 - accuracy: 0.7346 - val_loss: 1.3742 - val_accuracy: 0.4794\n",
      "Epoch 393/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7730 - accuracy: 0.7529 - val_loss: 1.3479 - val_accuracy: 0.4794\n",
      "Epoch 394/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/765 [==============================] - 3s 5ms/step - loss: 0.7728 - accuracy: 0.7399 - val_loss: 1.3372 - val_accuracy: 0.5103\n",
      "Epoch 395/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.7689 - accuracy: 0.7386 - val_loss: 1.3911 - val_accuracy: 0.4485\n",
      "Epoch 396/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7621 - accuracy: 0.7425 - val_loss: 1.3648 - val_accuracy: 0.4691\n",
      "Epoch 397/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7563 - accuracy: 0.7556 - val_loss: 1.4127 - val_accuracy: 0.4381\n",
      "Epoch 398/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.7661 - accuracy: 0.7359 - val_loss: 1.3737 - val_accuracy: 0.4536\n",
      "Epoch 399/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7653 - accuracy: 0.7373 - val_loss: 1.4262 - val_accuracy: 0.4278\n",
      "Epoch 400/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7618 - accuracy: 0.7621 - val_loss: 1.3266 - val_accuracy: 0.4897\n",
      "Epoch 401/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7599 - accuracy: 0.7516 - val_loss: 1.3551 - val_accuracy: 0.5000\n",
      "Epoch 402/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7490 - accuracy: 0.7595 - val_loss: 1.4792 - val_accuracy: 0.4485\n",
      "Epoch 403/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.7497 - accuracy: 0.7569 - val_loss: 1.3382 - val_accuracy: 0.4742\n",
      "Epoch 404/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.7613 - accuracy: 0.7608 - val_loss: 1.3079 - val_accuracy: 0.5155\n",
      "Epoch 405/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7557 - accuracy: 0.7412 - val_loss: 1.3005 - val_accuracy: 0.5103\n",
      "Epoch 406/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7493 - accuracy: 0.7464 - val_loss: 1.3518 - val_accuracy: 0.4794\n",
      "Epoch 407/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.7453 - accuracy: 0.7542 - val_loss: 1.3308 - val_accuracy: 0.5155\n",
      "Epoch 408/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.7487 - accuracy: 0.7660 - val_loss: 1.3779 - val_accuracy: 0.4948\n",
      "Epoch 409/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7382 - accuracy: 0.7451 - val_loss: 1.3207 - val_accuracy: 0.5258\n",
      "Epoch 410/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.7480 - accuracy: 0.7503 - val_loss: 1.3511 - val_accuracy: 0.4845\n",
      "Epoch 411/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.7372 - accuracy: 0.7569 - val_loss: 1.3557 - val_accuracy: 0.4897\n",
      "Epoch 412/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7312 - accuracy: 0.7595 - val_loss: 1.3214 - val_accuracy: 0.5309\n",
      "Epoch 413/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7306 - accuracy: 0.7516 - val_loss: 1.3279 - val_accuracy: 0.5000\n",
      "Epoch 414/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7278 - accuracy: 0.7569 - val_loss: 1.2991 - val_accuracy: 0.5206\n",
      "Epoch 415/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.7354 - accuracy: 0.7595 - val_loss: 1.3780 - val_accuracy: 0.4639\n",
      "Epoch 416/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7267 - accuracy: 0.7477 - val_loss: 1.3865 - val_accuracy: 0.4742\n",
      "Epoch 417/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.7336 - accuracy: 0.7634 - val_loss: 1.3570 - val_accuracy: 0.4948\n",
      "Epoch 418/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7245 - accuracy: 0.7569 - val_loss: 1.3686 - val_accuracy: 0.4588\n",
      "Epoch 419/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7268 - accuracy: 0.7542 - val_loss: 1.3946 - val_accuracy: 0.4794\n",
      "Epoch 420/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 0.7233 - accuracy: 0.7516 - val_loss: 1.4027 - val_accuracy: 0.4691\n",
      "Epoch 421/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7190 - accuracy: 0.7595 - val_loss: 1.3175 - val_accuracy: 0.5000\n",
      "Epoch 422/700\n",
      "765/765 [==============================] - 5s 6ms/step - loss: 0.7144 - accuracy: 0.7542 - val_loss: 1.3264 - val_accuracy: 0.4948\n",
      "Epoch 423/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 0.7126 - accuracy: 0.7739 - val_loss: 1.3964 - val_accuracy: 0.4948\n",
      "Epoch 424/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 0.7162 - accuracy: 0.7556 - val_loss: 1.3935 - val_accuracy: 0.4588\n",
      "Epoch 425/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.7162 - accuracy: 0.7817 - val_loss: 1.4406 - val_accuracy: 0.4278\n",
      "Epoch 426/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.7132 - accuracy: 0.7608 - val_loss: 1.3640 - val_accuracy: 0.4897\n",
      "Epoch 427/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.7118 - accuracy: 0.7660 - val_loss: 1.3033 - val_accuracy: 0.5103\n",
      "Epoch 428/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.7148 - accuracy: 0.7634 - val_loss: 1.3354 - val_accuracy: 0.4897\n",
      "Epoch 429/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.7097 - accuracy: 0.7595 - val_loss: 1.4323 - val_accuracy: 0.4639\n",
      "Epoch 430/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.6980 - accuracy: 0.7699 - val_loss: 1.3844 - val_accuracy: 0.4588\n",
      "Epoch 431/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.6960 - accuracy: 0.7673 - val_loss: 1.3076 - val_accuracy: 0.5206\n",
      "Epoch 432/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.7075 - accuracy: 0.7699 - val_loss: 1.3260 - val_accuracy: 0.4794\n",
      "Epoch 433/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.6973 - accuracy: 0.7712 - val_loss: 1.3313 - val_accuracy: 0.4742\n",
      "Epoch 434/700\n",
      "765/765 [==============================] - 5s 6ms/step - loss: 0.6989 - accuracy: 0.7739 - val_loss: 1.3527 - val_accuracy: 0.4691\n",
      "Epoch 435/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6958 - accuracy: 0.7621 - val_loss: 1.3499 - val_accuracy: 0.4948\n",
      "Epoch 436/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 0.6872 - accuracy: 0.7621 - val_loss: 1.3355 - val_accuracy: 0.5000\n",
      "Epoch 437/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 0.6951 - accuracy: 0.7791 - val_loss: 1.3700 - val_accuracy: 0.5000\n",
      "Epoch 438/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 0.6967 - accuracy: 0.7856 - val_loss: 1.4044 - val_accuracy: 0.4897\n",
      "Epoch 439/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6914 - accuracy: 0.7621 - val_loss: 1.3463 - val_accuracy: 0.4845\n",
      "Epoch 440/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6863 - accuracy: 0.7712 - val_loss: 1.3381 - val_accuracy: 0.5000\n",
      "Epoch 441/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.6885 - accuracy: 0.7856 - val_loss: 1.3800 - val_accuracy: 0.4639\n",
      "Epoch 442/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6837 - accuracy: 0.7752 - val_loss: 1.4159 - val_accuracy: 0.4536\n",
      "Epoch 443/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6879 - accuracy: 0.7752 - val_loss: 1.3358 - val_accuracy: 0.4639\n",
      "Epoch 444/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6794 - accuracy: 0.7856 - val_loss: 1.3275 - val_accuracy: 0.5052\n",
      "Epoch 445/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6772 - accuracy: 0.7856 - val_loss: 1.3468 - val_accuracy: 0.5155\n",
      "Epoch 446/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6722 - accuracy: 0.7895 - val_loss: 1.4805 - val_accuracy: 0.4381\n",
      "Epoch 447/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6687 - accuracy: 0.7739 - val_loss: 1.4651 - val_accuracy: 0.4381\n",
      "Epoch 448/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6661 - accuracy: 0.7804 - val_loss: 1.4273 - val_accuracy: 0.4227\n",
      "Epoch 449/700\n",
      "765/765 [==============================] - 6s 8ms/step - loss: 0.6692 - accuracy: 0.7725 - val_loss: 1.3557 - val_accuracy: 0.5103\n",
      "Epoch 450/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/765 [==============================] - 5s 6ms/step - loss: 0.6748 - accuracy: 0.7817 - val_loss: 1.3340 - val_accuracy: 0.4948\n",
      "Epoch 451/700\n",
      "765/765 [==============================] - 5s 6ms/step - loss: 0.6667 - accuracy: 0.7673 - val_loss: 1.3755 - val_accuracy: 0.4691\n",
      "Epoch 452/700\n",
      "765/765 [==============================] - 5s 6ms/step - loss: 0.6623 - accuracy: 0.7739 - val_loss: 1.4211 - val_accuracy: 0.4742\n",
      "Epoch 453/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 0.6711 - accuracy: 0.7739 - val_loss: 1.3730 - val_accuracy: 0.5155\n",
      "Epoch 454/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6634 - accuracy: 0.7843 - val_loss: 1.3817 - val_accuracy: 0.4948\n",
      "Epoch 455/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6604 - accuracy: 0.7804 - val_loss: 1.4044 - val_accuracy: 0.4742\n",
      "Epoch 456/700\n",
      "765/765 [==============================] - 5s 7ms/step - loss: 0.6619 - accuracy: 0.7895 - val_loss: 1.5830 - val_accuracy: 0.4072\n",
      "Epoch 457/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6564 - accuracy: 0.7908 - val_loss: 1.3566 - val_accuracy: 0.4794\n",
      "Epoch 458/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6605 - accuracy: 0.7817 - val_loss: 1.3481 - val_accuracy: 0.4691\n",
      "Epoch 459/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6659 - accuracy: 0.7686 - val_loss: 1.3429 - val_accuracy: 0.4948\n",
      "Epoch 460/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6597 - accuracy: 0.7791 - val_loss: 1.4409 - val_accuracy: 0.4381\n",
      "Epoch 461/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6532 - accuracy: 0.7961 - val_loss: 1.3671 - val_accuracy: 0.4948\n",
      "Epoch 462/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6542 - accuracy: 0.7882 - val_loss: 1.3852 - val_accuracy: 0.4536\n",
      "Epoch 463/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6398 - accuracy: 0.7895 - val_loss: 1.3677 - val_accuracy: 0.4588\n",
      "Epoch 464/700\n",
      "765/765 [==============================] - 5s 7ms/step - loss: 0.6470 - accuracy: 0.8026 - val_loss: 1.3866 - val_accuracy: 0.4897\n",
      "Epoch 465/700\n",
      "765/765 [==============================] - 4s 6ms/step - loss: 0.6524 - accuracy: 0.7895 - val_loss: 1.4209 - val_accuracy: 0.4742\n",
      "Epoch 466/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6441 - accuracy: 0.7869 - val_loss: 1.4492 - val_accuracy: 0.4845\n",
      "Epoch 467/700\n",
      "765/765 [==============================] - 5s 7ms/step - loss: 0.6428 - accuracy: 0.7948 - val_loss: 1.3396 - val_accuracy: 0.5155\n",
      "Epoch 468/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6333 - accuracy: 0.8000 - val_loss: 1.3545 - val_accuracy: 0.4691\n",
      "Epoch 469/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6451 - accuracy: 0.7895 - val_loss: 1.4145 - val_accuracy: 0.4381\n",
      "Epoch 470/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6441 - accuracy: 0.7804 - val_loss: 1.3395 - val_accuracy: 0.5103\n",
      "Epoch 471/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.6376 - accuracy: 0.8013 - val_loss: 1.3513 - val_accuracy: 0.4948\n",
      "Epoch 472/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6333 - accuracy: 0.7935 - val_loss: 1.3380 - val_accuracy: 0.5258\n",
      "Epoch 473/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6249 - accuracy: 0.7922 - val_loss: 1.3415 - val_accuracy: 0.5052\n",
      "Epoch 474/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6286 - accuracy: 0.7908 - val_loss: 1.3668 - val_accuracy: 0.4948\n",
      "Epoch 475/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6324 - accuracy: 0.8026 - val_loss: 1.3390 - val_accuracy: 0.4742\n",
      "Epoch 476/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6328 - accuracy: 0.8013 - val_loss: 1.3668 - val_accuracy: 0.5000\n",
      "Epoch 477/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.6230 - accuracy: 0.8118 - val_loss: 1.3551 - val_accuracy: 0.5000\n",
      "Epoch 478/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6212 - accuracy: 0.8105 - val_loss: 1.3596 - val_accuracy: 0.5155\n",
      "Epoch 479/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6242 - accuracy: 0.7935 - val_loss: 1.3846 - val_accuracy: 0.4691\n",
      "Epoch 480/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6133 - accuracy: 0.8052 - val_loss: 1.4132 - val_accuracy: 0.4433\n",
      "Epoch 481/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6207 - accuracy: 0.7935 - val_loss: 1.3235 - val_accuracy: 0.5155\n",
      "Epoch 482/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6119 - accuracy: 0.8131 - val_loss: 1.3640 - val_accuracy: 0.4794\n",
      "Epoch 483/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6205 - accuracy: 0.8039 - val_loss: 1.3095 - val_accuracy: 0.5155\n",
      "Epoch 484/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.6112 - accuracy: 0.8026 - val_loss: 1.3812 - val_accuracy: 0.4742\n",
      "Epoch 485/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6024 - accuracy: 0.8105 - val_loss: 1.3809 - val_accuracy: 0.4948\n",
      "Epoch 486/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6098 - accuracy: 0.8000 - val_loss: 1.3575 - val_accuracy: 0.4897\n",
      "Epoch 487/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6078 - accuracy: 0.8000 - val_loss: 1.3745 - val_accuracy: 0.4691\n",
      "Epoch 488/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6151 - accuracy: 0.7948 - val_loss: 1.3628 - val_accuracy: 0.5052\n",
      "Epoch 489/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6070 - accuracy: 0.8118 - val_loss: 1.4091 - val_accuracy: 0.4691\n",
      "Epoch 490/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6024 - accuracy: 0.8105 - val_loss: 1.3256 - val_accuracy: 0.5103\n",
      "Epoch 491/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6056 - accuracy: 0.8092 - val_loss: 1.3494 - val_accuracy: 0.4845\n",
      "Epoch 492/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.6030 - accuracy: 0.7961 - val_loss: 1.4364 - val_accuracy: 0.4433\n",
      "Epoch 493/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5917 - accuracy: 0.8131 - val_loss: 1.4154 - val_accuracy: 0.4639\n",
      "Epoch 494/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5957 - accuracy: 0.8144 - val_loss: 1.3770 - val_accuracy: 0.5206\n",
      "Epoch 495/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5917 - accuracy: 0.8052 - val_loss: 1.4166 - val_accuracy: 0.4948\n",
      "Epoch 496/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5846 - accuracy: 0.8235 - val_loss: 1.4101 - val_accuracy: 0.4536\n",
      "Epoch 497/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5938 - accuracy: 0.8222 - val_loss: 1.4221 - val_accuracy: 0.4433\n",
      "Epoch 498/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5891 - accuracy: 0.8092 - val_loss: 1.3556 - val_accuracy: 0.4794\n",
      "Epoch 499/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5872 - accuracy: 0.8209 - val_loss: 1.4035 - val_accuracy: 0.4742\n",
      "Epoch 500/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5824 - accuracy: 0.8065 - val_loss: 1.4335 - val_accuracy: 0.4691\n",
      "Epoch 501/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5886 - accuracy: 0.8183 - val_loss: 1.3948 - val_accuracy: 0.4742\n",
      "Epoch 502/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5807 - accuracy: 0.8144 - val_loss: 1.3819 - val_accuracy: 0.4948\n",
      "Epoch 503/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5876 - accuracy: 0.8092 - val_loss: 1.4126 - val_accuracy: 0.4948\n",
      "Epoch 504/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5784 - accuracy: 0.8144 - val_loss: 1.4674 - val_accuracy: 0.4639\n",
      "Epoch 505/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5856 - accuracy: 0.8026 - val_loss: 1.3971 - val_accuracy: 0.5052\n",
      "Epoch 506/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5780 - accuracy: 0.8183 - val_loss: 1.4454 - val_accuracy: 0.4433\n",
      "Epoch 507/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5727 - accuracy: 0.8209 - val_loss: 1.4377 - val_accuracy: 0.4588\n",
      "Epoch 508/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5797 - accuracy: 0.8092 - val_loss: 1.3691 - val_accuracy: 0.5000\n",
      "Epoch 509/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5704 - accuracy: 0.8222 - val_loss: 1.4163 - val_accuracy: 0.5052\n",
      "Epoch 510/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5657 - accuracy: 0.8248 - val_loss: 1.4869 - val_accuracy: 0.4639\n",
      "Epoch 511/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5718 - accuracy: 0.8144 - val_loss: 1.3901 - val_accuracy: 0.4948\n",
      "Epoch 512/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5718 - accuracy: 0.8288 - val_loss: 1.3703 - val_accuracy: 0.5052\n",
      "Epoch 513/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5729 - accuracy: 0.8013 - val_loss: 1.3907 - val_accuracy: 0.4948\n",
      "Epoch 514/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5639 - accuracy: 0.8209 - val_loss: 1.4456 - val_accuracy: 0.4948\n",
      "Epoch 515/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5627 - accuracy: 0.8418 - val_loss: 1.3850 - val_accuracy: 0.4794\n",
      "Epoch 516/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5603 - accuracy: 0.8275 - val_loss: 1.4274 - val_accuracy: 0.4742\n",
      "Epoch 517/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5596 - accuracy: 0.8261 - val_loss: 1.4162 - val_accuracy: 0.5052\n",
      "Epoch 518/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5604 - accuracy: 0.8366 - val_loss: 1.4110 - val_accuracy: 0.5000\n",
      "Epoch 519/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5538 - accuracy: 0.8235 - val_loss: 1.5090 - val_accuracy: 0.4691\n",
      "Epoch 520/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5617 - accuracy: 0.8065 - val_loss: 1.4204 - val_accuracy: 0.4639\n",
      "Epoch 521/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5560 - accuracy: 0.8118 - val_loss: 1.4000 - val_accuracy: 0.5206\n",
      "Epoch 522/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5529 - accuracy: 0.8366 - val_loss: 1.4091 - val_accuracy: 0.4691\n",
      "Epoch 523/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5354 - accuracy: 0.8314 - val_loss: 1.5270 - val_accuracy: 0.4691\n",
      "Epoch 524/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5445 - accuracy: 0.8340 - val_loss: 1.4608 - val_accuracy: 0.5155\n",
      "Epoch 525/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5511 - accuracy: 0.8301 - val_loss: 1.3831 - val_accuracy: 0.5155\n",
      "Epoch 526/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5454 - accuracy: 0.8275 - val_loss: 1.3834 - val_accuracy: 0.4845\n",
      "Epoch 527/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5311 - accuracy: 0.8288 - val_loss: 1.4780 - val_accuracy: 0.4536\n",
      "Epoch 528/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5426 - accuracy: 0.8275 - val_loss: 1.4497 - val_accuracy: 0.5103\n",
      "Epoch 529/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5411 - accuracy: 0.8261 - val_loss: 1.3786 - val_accuracy: 0.4742\n",
      "Epoch 530/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5383 - accuracy: 0.8392 - val_loss: 1.4602 - val_accuracy: 0.4691\n",
      "Epoch 531/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5306 - accuracy: 0.8471 - val_loss: 1.4129 - val_accuracy: 0.4948\n",
      "Epoch 532/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5286 - accuracy: 0.8327 - val_loss: 1.4487 - val_accuracy: 0.4691\n",
      "Epoch 533/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5234 - accuracy: 0.8444 - val_loss: 1.4220 - val_accuracy: 0.4845\n",
      "Epoch 534/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5343 - accuracy: 0.8301 - val_loss: 1.4382 - val_accuracy: 0.4691\n",
      "Epoch 535/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5347 - accuracy: 0.8340 - val_loss: 1.4009 - val_accuracy: 0.4691\n",
      "Epoch 536/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5253 - accuracy: 0.8353 - val_loss: 1.4562 - val_accuracy: 0.4433\n",
      "Epoch 537/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5336 - accuracy: 0.8353 - val_loss: 1.3807 - val_accuracy: 0.5052\n",
      "Epoch 538/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5188 - accuracy: 0.8340 - val_loss: 1.4629 - val_accuracy: 0.4742\n",
      "Epoch 539/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5220 - accuracy: 0.8444 - val_loss: 1.3744 - val_accuracy: 0.4897\n",
      "Epoch 540/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5232 - accuracy: 0.8392 - val_loss: 1.3970 - val_accuracy: 0.4897\n",
      "Epoch 541/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5153 - accuracy: 0.8418 - val_loss: 1.4079 - val_accuracy: 0.4845\n",
      "Epoch 542/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5101 - accuracy: 0.8366 - val_loss: 1.4213 - val_accuracy: 0.4485\n",
      "Epoch 543/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5188 - accuracy: 0.8392 - val_loss: 1.4014 - val_accuracy: 0.4794\n",
      "Epoch 544/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5140 - accuracy: 0.8340 - val_loss: 1.4473 - val_accuracy: 0.4588\n",
      "Epoch 545/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5141 - accuracy: 0.8405 - val_loss: 1.4838 - val_accuracy: 0.4742\n",
      "Epoch 546/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5084 - accuracy: 0.8392 - val_loss: 1.4170 - val_accuracy: 0.4794\n",
      "Epoch 547/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5005 - accuracy: 0.8392 - val_loss: 1.5307 - val_accuracy: 0.4588\n",
      "Epoch 548/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.5131 - accuracy: 0.8366 - val_loss: 1.4157 - val_accuracy: 0.5000\n",
      "Epoch 549/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5044 - accuracy: 0.8484 - val_loss: 1.4383 - val_accuracy: 0.4433\n",
      "Epoch 550/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5035 - accuracy: 0.8379 - val_loss: 1.4384 - val_accuracy: 0.4742\n",
      "Epoch 551/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.5035 - accuracy: 0.8418 - val_loss: 1.5117 - val_accuracy: 0.4691\n",
      "Epoch 552/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4988 - accuracy: 0.8562 - val_loss: 1.4163 - val_accuracy: 0.4742\n",
      "Epoch 553/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4944 - accuracy: 0.8510 - val_loss: 1.4490 - val_accuracy: 0.4742\n",
      "Epoch 554/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4907 - accuracy: 0.8549 - val_loss: 1.4670 - val_accuracy: 0.4742\n",
      "Epoch 555/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4962 - accuracy: 0.8418 - val_loss: 1.5134 - val_accuracy: 0.4536\n",
      "Epoch 556/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4946 - accuracy: 0.8523 - val_loss: 1.4559 - val_accuracy: 0.4897\n",
      "Epoch 557/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4930 - accuracy: 0.8510 - val_loss: 1.4290 - val_accuracy: 0.4639\n",
      "Epoch 558/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4941 - accuracy: 0.8641 - val_loss: 1.3927 - val_accuracy: 0.5000\n",
      "Epoch 559/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4861 - accuracy: 0.8575 - val_loss: 1.4871 - val_accuracy: 0.4485\n",
      "Epoch 560/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4865 - accuracy: 0.8458 - val_loss: 1.4439 - val_accuracy: 0.4794\n",
      "Epoch 561/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4994 - accuracy: 0.8418 - val_loss: 1.4259 - val_accuracy: 0.4948\n",
      "Epoch 562/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4784 - accuracy: 0.8458 - val_loss: 1.4516 - val_accuracy: 0.4588\n",
      "Epoch 563/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.4755 - accuracy: 0.8523 - val_loss: 1.4560 - val_accuracy: 0.4948\n",
      "Epoch 564/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4760 - accuracy: 0.8588 - val_loss: 1.4853 - val_accuracy: 0.4639\n",
      "Epoch 565/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.4813 - accuracy: 0.8536 - val_loss: 1.4234 - val_accuracy: 0.4691\n",
      "Epoch 566/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.4731 - accuracy: 0.8366 - val_loss: 1.4795 - val_accuracy: 0.4845\n",
      "Epoch 567/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4790 - accuracy: 0.8471 - val_loss: 1.4292 - val_accuracy: 0.4948\n",
      "Epoch 568/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4771 - accuracy: 0.8536 - val_loss: 1.4589 - val_accuracy: 0.4639\n",
      "Epoch 569/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.4624 - accuracy: 0.8510 - val_loss: 1.3885 - val_accuracy: 0.5258\n",
      "Epoch 570/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4750 - accuracy: 0.8575 - val_loss: 1.4376 - val_accuracy: 0.4433\n",
      "Epoch 571/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4661 - accuracy: 0.8614 - val_loss: 1.4638 - val_accuracy: 0.4845\n",
      "Epoch 572/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.4619 - accuracy: 0.8588 - val_loss: 1.4513 - val_accuracy: 0.4897\n",
      "Epoch 573/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4655 - accuracy: 0.8562 - val_loss: 1.4409 - val_accuracy: 0.4691\n",
      "Epoch 574/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4625 - accuracy: 0.8627 - val_loss: 1.5139 - val_accuracy: 0.4639\n",
      "Epoch 575/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.4594 - accuracy: 0.8562 - val_loss: 1.4538 - val_accuracy: 0.4742\n",
      "Epoch 576/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4565 - accuracy: 0.8641 - val_loss: 1.4243 - val_accuracy: 0.5103\n",
      "Epoch 577/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4558 - accuracy: 0.8680 - val_loss: 1.4048 - val_accuracy: 0.5103\n",
      "Epoch 578/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4474 - accuracy: 0.8654 - val_loss: 1.4631 - val_accuracy: 0.5000\n",
      "Epoch 579/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4572 - accuracy: 0.8614 - val_loss: 1.4273 - val_accuracy: 0.4794\n",
      "Epoch 580/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4576 - accuracy: 0.8641 - val_loss: 1.4527 - val_accuracy: 0.4897\n",
      "Epoch 581/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4480 - accuracy: 0.8732 - val_loss: 1.4542 - val_accuracy: 0.4588\n",
      "Epoch 582/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4478 - accuracy: 0.8667 - val_loss: 1.4774 - val_accuracy: 0.4742\n",
      "Epoch 583/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.4500 - accuracy: 0.8641 - val_loss: 1.4562 - val_accuracy: 0.5000\n",
      "Epoch 584/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4509 - accuracy: 0.8627 - val_loss: 1.5417 - val_accuracy: 0.4588\n",
      "Epoch 585/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4439 - accuracy: 0.8732 - val_loss: 1.5111 - val_accuracy: 0.4536\n",
      "Epoch 586/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4486 - accuracy: 0.8719 - val_loss: 1.4611 - val_accuracy: 0.4691\n",
      "Epoch 587/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4419 - accuracy: 0.8771 - val_loss: 1.4482 - val_accuracy: 0.4948\n",
      "Epoch 588/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4460 - accuracy: 0.8680 - val_loss: 1.4870 - val_accuracy: 0.4639\n",
      "Epoch 589/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4353 - accuracy: 0.8824 - val_loss: 1.4486 - val_accuracy: 0.5000\n",
      "Epoch 590/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4327 - accuracy: 0.8771 - val_loss: 1.4781 - val_accuracy: 0.4845\n",
      "Epoch 591/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4348 - accuracy: 0.8758 - val_loss: 1.4402 - val_accuracy: 0.4845\n",
      "Epoch 592/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4231 - accuracy: 0.8706 - val_loss: 1.5533 - val_accuracy: 0.4588\n",
      "Epoch 593/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4288 - accuracy: 0.8810 - val_loss: 1.5241 - val_accuracy: 0.4639\n",
      "Epoch 594/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4289 - accuracy: 0.8732 - val_loss: 1.4991 - val_accuracy: 0.4588\n",
      "Epoch 595/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.4223 - accuracy: 0.8902 - val_loss: 1.6106 - val_accuracy: 0.4639\n",
      "Epoch 596/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4285 - accuracy: 0.8706 - val_loss: 1.5137 - val_accuracy: 0.4330\n",
      "Epoch 597/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4263 - accuracy: 0.8719 - val_loss: 1.4685 - val_accuracy: 0.4845\n",
      "Epoch 598/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4262 - accuracy: 0.8654 - val_loss: 1.5361 - val_accuracy: 0.4330\n",
      "Epoch 599/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4230 - accuracy: 0.8784 - val_loss: 1.4742 - val_accuracy: 0.4536\n",
      "Epoch 600/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4117 - accuracy: 0.8797 - val_loss: 1.6559 - val_accuracy: 0.4072\n",
      "Epoch 601/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4226 - accuracy: 0.8810 - val_loss: 1.5093 - val_accuracy: 0.4691\n",
      "Epoch 602/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4196 - accuracy: 0.8784 - val_loss: 1.5408 - val_accuracy: 0.4588\n",
      "Epoch 603/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4117 - accuracy: 0.8784 - val_loss: 1.5576 - val_accuracy: 0.4742\n",
      "Epoch 604/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4021 - accuracy: 0.8719 - val_loss: 1.4377 - val_accuracy: 0.5000\n",
      "Epoch 605/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.4140 - accuracy: 0.8706 - val_loss: 1.4984 - val_accuracy: 0.4794\n",
      "Epoch 606/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4155 - accuracy: 0.8693 - val_loss: 1.5190 - val_accuracy: 0.4691\n",
      "Epoch 607/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4110 - accuracy: 0.8850 - val_loss: 1.4922 - val_accuracy: 0.4897\n",
      "Epoch 608/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4056 - accuracy: 0.8915 - val_loss: 1.5082 - val_accuracy: 0.4742\n",
      "Epoch 609/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3977 - accuracy: 0.8876 - val_loss: 1.4811 - val_accuracy: 0.4639\n",
      "Epoch 610/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3975 - accuracy: 0.8837 - val_loss: 1.5274 - val_accuracy: 0.5000\n",
      "Epoch 611/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3977 - accuracy: 0.9020 - val_loss: 1.5432 - val_accuracy: 0.4948\n",
      "Epoch 612/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4009 - accuracy: 0.8824 - val_loss: 1.5202 - val_accuracy: 0.5000\n",
      "Epoch 613/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.4006 - accuracy: 0.8889 - val_loss: 1.5736 - val_accuracy: 0.4330\n",
      "Epoch 614/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3975 - accuracy: 0.8889 - val_loss: 1.4601 - val_accuracy: 0.4845\n",
      "Epoch 615/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.4003 - accuracy: 0.8863 - val_loss: 1.5355 - val_accuracy: 0.4433\n",
      "Epoch 616/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3948 - accuracy: 0.8902 - val_loss: 1.5447 - val_accuracy: 0.5000\n",
      "Epoch 617/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3919 - accuracy: 0.8784 - val_loss: 1.5159 - val_accuracy: 0.4897\n",
      "Epoch 618/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3840 - accuracy: 0.8941 - val_loss: 1.5110 - val_accuracy: 0.4691\n",
      "Epoch 619/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3871 - accuracy: 0.8967 - val_loss: 1.5711 - val_accuracy: 0.4588\n",
      "Epoch 620/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3744 - accuracy: 0.8967 - val_loss: 1.5372 - val_accuracy: 0.4794\n",
      "Epoch 621/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3829 - accuracy: 0.8902 - val_loss: 1.4988 - val_accuracy: 0.5052\n",
      "Epoch 622/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3827 - accuracy: 0.8850 - val_loss: 1.5545 - val_accuracy: 0.4485\n",
      "Epoch 623/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3788 - accuracy: 0.8980 - val_loss: 1.4860 - val_accuracy: 0.4742\n",
      "Epoch 624/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.3903 - accuracy: 0.8915 - val_loss: 1.5680 - val_accuracy: 0.5052\n",
      "Epoch 625/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.3848 - accuracy: 0.8902 - val_loss: 1.5325 - val_accuracy: 0.4485\n",
      "Epoch 626/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3784 - accuracy: 0.8928 - val_loss: 1.5378 - val_accuracy: 0.4845\n",
      "Epoch 627/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3800 - accuracy: 0.8928 - val_loss: 1.5538 - val_accuracy: 0.4691\n",
      "Epoch 628/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3748 - accuracy: 0.8967 - val_loss: 1.6281 - val_accuracy: 0.4433\n",
      "Epoch 629/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3768 - accuracy: 0.8941 - val_loss: 1.5195 - val_accuracy: 0.4742\n",
      "Epoch 630/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3683 - accuracy: 0.8915 - val_loss: 1.5893 - val_accuracy: 0.4433\n",
      "Epoch 631/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3740 - accuracy: 0.8954 - val_loss: 1.5752 - val_accuracy: 0.4691\n",
      "Epoch 632/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3728 - accuracy: 0.8863 - val_loss: 1.5833 - val_accuracy: 0.4433\n",
      "Epoch 633/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3739 - accuracy: 0.8980 - val_loss: 1.5090 - val_accuracy: 0.4897\n",
      "Epoch 634/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3592 - accuracy: 0.9046 - val_loss: 1.5399 - val_accuracy: 0.4485\n",
      "Epoch 635/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3670 - accuracy: 0.9046 - val_loss: 1.4916 - val_accuracy: 0.5000\n",
      "Epoch 636/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3636 - accuracy: 0.9033 - val_loss: 1.5526 - val_accuracy: 0.5000\n",
      "Epoch 637/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3620 - accuracy: 0.9033 - val_loss: 1.5580 - val_accuracy: 0.4845\n",
      "Epoch 638/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3514 - accuracy: 0.9033 - val_loss: 1.5992 - val_accuracy: 0.4433\n",
      "Epoch 639/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3588 - accuracy: 0.9046 - val_loss: 1.5578 - val_accuracy: 0.4485\n",
      "Epoch 640/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3566 - accuracy: 0.8993 - val_loss: 1.6051 - val_accuracy: 0.4588\n",
      "Epoch 641/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.3531 - accuracy: 0.9072 - val_loss: 1.5919 - val_accuracy: 0.4278\n",
      "Epoch 642/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.3555 - accuracy: 0.9033 - val_loss: 1.5679 - val_accuracy: 0.4691\n",
      "Epoch 643/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3389 - accuracy: 0.9137 - val_loss: 1.5011 - val_accuracy: 0.5103\n",
      "Epoch 644/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3495 - accuracy: 0.8993 - val_loss: 1.5785 - val_accuracy: 0.4588\n",
      "Epoch 645/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3477 - accuracy: 0.8993 - val_loss: 1.6138 - val_accuracy: 0.4639\n",
      "Epoch 646/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3395 - accuracy: 0.9098 - val_loss: 1.5688 - val_accuracy: 0.4639\n",
      "Epoch 647/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3458 - accuracy: 0.9046 - val_loss: 1.5650 - val_accuracy: 0.4433\n",
      "Epoch 648/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3408 - accuracy: 0.9072 - val_loss: 1.5525 - val_accuracy: 0.5052\n",
      "Epoch 649/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3419 - accuracy: 0.9085 - val_loss: 1.5544 - val_accuracy: 0.4948\n",
      "Epoch 650/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3392 - accuracy: 0.9046 - val_loss: 1.5782 - val_accuracy: 0.4588\n",
      "Epoch 651/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3380 - accuracy: 0.9203 - val_loss: 1.5470 - val_accuracy: 0.4845\n",
      "Epoch 652/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.3377 - accuracy: 0.9150 - val_loss: 1.5846 - val_accuracy: 0.4794\n",
      "Epoch 653/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3382 - accuracy: 0.9085 - val_loss: 1.6063 - val_accuracy: 0.4742\n",
      "Epoch 654/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.3274 - accuracy: 0.9111 - val_loss: 1.6117 - val_accuracy: 0.4742\n",
      "Epoch 655/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.3345 - accuracy: 0.9124 - val_loss: 1.5644 - val_accuracy: 0.4639\n",
      "Epoch 656/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3358 - accuracy: 0.9059 - val_loss: 1.5396 - val_accuracy: 0.4639\n",
      "Epoch 657/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3167 - accuracy: 0.9255 - val_loss: 1.5949 - val_accuracy: 0.4794\n",
      "Epoch 658/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3327 - accuracy: 0.9137 - val_loss: 1.6148 - val_accuracy: 0.4794\n",
      "Epoch 659/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3256 - accuracy: 0.9163 - val_loss: 1.5437 - val_accuracy: 0.4691\n",
      "Epoch 660/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3242 - accuracy: 0.9020 - val_loss: 1.5816 - val_accuracy: 0.4691\n",
      "Epoch 661/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3335 - accuracy: 0.9046 - val_loss: 1.5998 - val_accuracy: 0.4536\n",
      "Epoch 662/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3204 - accuracy: 0.9190 - val_loss: 1.6401 - val_accuracy: 0.4794\n",
      "Epoch 663/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3281 - accuracy: 0.9098 - val_loss: 1.6190 - val_accuracy: 0.4433\n",
      "Epoch 664/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3216 - accuracy: 0.9085 - val_loss: 1.7126 - val_accuracy: 0.4227\n",
      "Epoch 665/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3298 - accuracy: 0.9085 - val_loss: 1.6129 - val_accuracy: 0.4485\n",
      "Epoch 666/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3137 - accuracy: 0.9176 - val_loss: 1.6489 - val_accuracy: 0.4691\n",
      "Epoch 667/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3096 - accuracy: 0.9137 - val_loss: 1.5787 - val_accuracy: 0.5103\n",
      "Epoch 668/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3167 - accuracy: 0.9137 - val_loss: 1.5931 - val_accuracy: 0.4742\n",
      "Epoch 669/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3204 - accuracy: 0.9229 - val_loss: 1.6072 - val_accuracy: 0.4897\n",
      "Epoch 670/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3026 - accuracy: 0.9281 - val_loss: 1.6098 - val_accuracy: 0.4588\n",
      "Epoch 671/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3055 - accuracy: 0.9307 - val_loss: 1.6530 - val_accuracy: 0.4639\n",
      "Epoch 672/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3076 - accuracy: 0.9307 - val_loss: 1.6021 - val_accuracy: 0.5000\n",
      "Epoch 673/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.2957 - accuracy: 0.9346 - val_loss: 1.5781 - val_accuracy: 0.4639\n",
      "Epoch 674/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "765/765 [==============================] - 3s 4ms/step - loss: 0.3039 - accuracy: 0.9255 - val_loss: 1.6667 - val_accuracy: 0.4381\n",
      "Epoch 675/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.2968 - accuracy: 0.9229 - val_loss: 1.7467 - val_accuracy: 0.4072\n",
      "Epoch 676/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.2956 - accuracy: 0.9229 - val_loss: 1.6391 - val_accuracy: 0.4948\n",
      "Epoch 677/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.3008 - accuracy: 0.9307 - val_loss: 1.6254 - val_accuracy: 0.4588\n",
      "Epoch 678/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.2941 - accuracy: 0.9320 - val_loss: 1.6616 - val_accuracy: 0.4124\n",
      "Epoch 679/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.2936 - accuracy: 0.9320 - val_loss: 1.6089 - val_accuracy: 0.4742\n",
      "Epoch 680/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.3000 - accuracy: 0.9268 - val_loss: 1.6465 - val_accuracy: 0.4794\n",
      "Epoch 681/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.2922 - accuracy: 0.9281 - val_loss: 1.6065 - val_accuracy: 0.4536\n",
      "Epoch 682/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.2897 - accuracy: 0.9255 - val_loss: 1.7420 - val_accuracy: 0.4536\n",
      "Epoch 683/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.2910 - accuracy: 0.9333 - val_loss: 1.6272 - val_accuracy: 0.4433\n",
      "Epoch 684/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.2896 - accuracy: 0.9333 - val_loss: 1.6537 - val_accuracy: 0.4485\n",
      "Epoch 685/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.2852 - accuracy: 0.9373 - val_loss: 1.6017 - val_accuracy: 0.4794\n",
      "Epoch 686/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.2860 - accuracy: 0.9320 - val_loss: 1.6312 - val_accuracy: 0.4742\n",
      "Epoch 687/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.2865 - accuracy: 0.9242 - val_loss: 1.6998 - val_accuracy: 0.4536\n",
      "Epoch 688/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.2869 - accuracy: 0.9307 - val_loss: 1.6118 - val_accuracy: 0.4742\n",
      "Epoch 689/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.2815 - accuracy: 0.9281 - val_loss: 1.5589 - val_accuracy: 0.4948\n",
      "Epoch 690/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.2845 - accuracy: 0.9346 - val_loss: 1.5917 - val_accuracy: 0.5052\n",
      "Epoch 691/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.2802 - accuracy: 0.9294 - val_loss: 1.6402 - val_accuracy: 0.4794\n",
      "Epoch 692/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.2828 - accuracy: 0.9242 - val_loss: 1.6531 - val_accuracy: 0.5000\n",
      "Epoch 693/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.2813 - accuracy: 0.9307 - val_loss: 1.6241 - val_accuracy: 0.4588\n",
      "Epoch 694/700\n",
      "765/765 [==============================] - 3s 4ms/step - loss: 0.2759 - accuracy: 0.9333 - val_loss: 1.6696 - val_accuracy: 0.4433\n",
      "Epoch 695/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.2735 - accuracy: 0.9268 - val_loss: 1.6076 - val_accuracy: 0.4845\n",
      "Epoch 696/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.2770 - accuracy: 0.9373 - val_loss: 1.6700 - val_accuracy: 0.4794\n",
      "Epoch 697/700\n",
      "765/765 [==============================] - 3s 5ms/step - loss: 0.2764 - accuracy: 0.9359 - val_loss: 1.6795 - val_accuracy: 0.4536\n",
      "Epoch 698/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.2700 - accuracy: 0.9346 - val_loss: 1.6472 - val_accuracy: 0.4742\n",
      "Epoch 699/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.2653 - accuracy: 0.9399 - val_loss: 1.6198 - val_accuracy: 0.4794\n",
      "Epoch 700/700\n",
      "765/765 [==============================] - 4s 5ms/step - loss: 0.2687 - accuracy: 0.9346 - val_loss: 1.6533 - val_accuracy: 0.4639\n"
     ]
    }
   ],
   "source": [
    "cnnhistory=model.fit(x_traincnn, y_train, batch_size=16, epochs=700, validation_data=(x_testcnn, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xd4VGX2wPHvSSeFQBJ6C733riI2FLCuBbGhroq6NlbXXd1V17L709Vd1957w66rKIooKFiohg7SIaEEQgoJpL+/P947mUkyaZDJTDLn8zx5ctvcOQOTe+59qxhjUEoppQBC/B2AUkqpwKFJQSmlVBlNCkoppcpoUlBKKVVGk4JSSqkymhSUUkqV0aSgVC2JyGsi8o9aHrtNRE452vMo1dA0KSillCqjSUEppVQZTQqqSXGKbW4XkZUikiciL4tIGxGZLSIHRWSuiLT0OP4sEVkjIlkiMl9E+nrsGyoiy53XvQdEVXivM0QkxXntTyIy6AhjvkZENonIARH5TETaO9tFRP4rIukiku18pgHOvskistaJLU1E/nRE/2BKVaBJQTVF5wETgF7AmcBs4K9AEvY7fzOAiPQCZgIzgFbAl8DnIhIhIhHAp8CbQALwgXNenNcOA14BrgUSgeeBz0Qksi6BishJwIPAFKAdsB1419l9KnC88zlaABcCGc6+l4FrjTFxwADgu7q8r1JV0aSgmqInjTF7jTFpwAJgkTHmV2NMAfAJMNQ57kLgC2PMN8aYIuDfQDPgGGAMEA48ZowpMsZ8CCzxeI9rgOeNMYuMMSXGmNeBAud1dXEJ8IoxZrkT353AWBFJBoqAOKAPIMaYdcaY3c7rioB+ItLcGJNpjFlex/dVyitNCqop2uuxfNjLeqyz3B57Zw6AMaYU2Al0cPalmfIjRm73WO4C3OYUHWWJSBbQyXldXVSMIRf7NNDBGPMd8BTwNLBXRF4QkebOoecBk4HtIvK9iIyt4/sq5ZUmBRXMdmEv7oAtw8de2NOA3UAHZ5tLZ4/lncA/jTEtPH6ijTEzjzKGGGxxVBqAMeYJY8xwoD+2GOl2Z/sSY8zZQGtsMdf7dXxfpbzSpKCC2fvA6SJysoiEA7dhi4B+An4GioGbRSRMRM4FRnm89kXgOhEZ7VQIx4jI6SISV8cY3gGuFJEhTn3E/2GLu7aJyEjn/OFAHpAPlDh1HpeISLxT7JUDlBzFv4NSZTQpqKBljNkAXAo8CezHVkqfaYwpNMYUAucCVwCZ2PqHjz1euxRbr/CUs3+Tc2xdY/gWuBv4CPt00h2Y6uxujk0+mdgipgxsvQfAZcA2EckBrnM+h1JHTXSSHaWUUi76pKCUUqqMJgWllFJlNCkopZQqo0lBKaVUmTB/B1BXSUlJJjk52d9hKKVUo7Js2bL9xphWNR3X6JJCcnIyS5cu9XcYSinVqIjI9pqP0uIjpZRSHjQpKKWUKqNJQSmlVJlGV6fgTVFREampqeTn5/s7FJ+LioqiY8eOhIeH+zsUpVQT5LOkICJRwA9ApPM+Hxpj/l7hmEjgDWA4dlyXC40x2+r6XqmpqcTFxZGcnEz5QS2bFmMMGRkZpKam0rVrV3+Ho5RqgnxZfFQAnGSMGQwMASaKSMUJSK4CMo0xPYD/Av86kjfKz88nMTGxSScEABEhMTExKJ6IlFL+4bOkYKxcZzXc+ak4+t7ZwOvO8ofAyXKEV/amnhBcguVzKqX8w6cVzSISKiIpQDrwjTFmUYVDOmAnK8EYUwxkYycYqXie6SKyVESW7tu374hiyS8qYU92PkUlpUf0eqWUCgY+TQrO3LVDgI7AKBEZUOEQb7e9lcbyNsa8YIwZYYwZ0apVjR3yvMovKiH9YD4lpfU/VHhWVhbPPPNMnV83efJksrKy6j0epZQ6Ug3SJNUYkwXMByZW2JWKnf4QEQkD4oEDvojBlX18MXtEVUmhpKT6ybC+/PJLWrRo4YOIlFLqyPgsKYhIKxFp4Sw3A04B1lc47DPgcmf5fOA746tZf1xl8T44+x133MHmzZsZMmQII0eO5MQTT+Tiiy9m4MCBAJxzzjkMHz6c/v3788ILL5S9Ljk5mf3797Nt2zb69u3LNddcQ//+/Tn11FM5fPhw/QeqlFI18GU/hXbA6yISik0+7xtjZonI/cBSY8xnwMvAmyKyCfuEMLXq09XOfZ+vYe2unErbS0oN+UUlNIsIJaSOlbX92jfn72f2r3L/Qw89xOrVq0lJSWH+/PmcfvrprF69uqzZ6CuvvEJCQgKHDx9m5MiRnHfeeSQmlq862bhxIzNnzuTFF19kypQpfPTRR1x6qc6wqJRqWD5LCsaYlcBQL9vv8VjOBy7wVQz+MmrUqHL9CJ544gk++eQTAHbu3MnGjRsrJYWuXbsyZMgQAIYPH862bdsaLF6llHJpEj2aPVV1R59zuIhtGXn0aB1LdIRvP3ZMTEzZ8vz585k7dy4///wz0dHRnHDCCV77GURGRpYth4aGavGRUsovgmbso7IqBR/UKcTFxXHw4EGv+7Kzs2nZsiXR0dGsX7+eX375pf4DUEqpetLknhT8ITExkWOPPZYBAwbQrFkz2rRpU7Zv4sSJPPfccwwaNIjevXszZkzFTt1KKRU4xFeNfXxlxIgRpuIkO+vWraNv377Vvi43v4gt+/Po3iqWmMjGnQtr83mVUsqTiCwzxoyo6bigKT5yaVwpUCmlGlbwJAVfVioopVQTETRJwZc9mpVSqqkImqSglFKqZpoUlFJKlQmapKBVCkopVbOgSQq+dKRDZwM89thjHDp0qJ4jUkqpIxM0ScEfQ2fXhiYFpVQgady9uOrEd2nBc+jsCRMm0Lp1a95//30KCgr43e9+x3333UdeXh5TpkwhNTWVkpIS7r77bvbu3cuuXbs48cQTSUpKYt68efUem1JK1UXTSwqz74A9qyptjjSGboUlRIWHQEgdH5DaDoRJD1W523Po7Dlz5vDhhx+yePFijDGcddZZ/PDDD+zbt4/27dvzxRdfAHZMpPj4eB599FHmzZtHUlJS3WJSSikfCJrio4YyZ84c5syZw9ChQxk2bBjr169n48aNDBw4kLlz5/KXv/yFBQsWEB8f7+9QlVKqkqb3pFDFHX1hUQlb9h6kc0I0LaIjfPb2xhjuvPNOrr322kr7li1bxpdffsmdd97Jqaeeyj333OPlDEop5T9B86Tgy4pmz6GzTzvtNF555RVyc3MBSEtLIz09nV27dhEdHc2ll17Kn/70J5YvX17ptUop5W9N70mhKj7MCp5DZ0+aNImLL76YsWPHAhAbG8tbb73Fpk2buP322wkJCSE8PJxnn30WgOnTpzNp0iTatWunFc1KKb8LmqGzC4tLWL/nIB1bRpMQ47vio4agQ2crpepKh86uRGo+RCmlglwQJQWXxvVkpJRSDanJJIWaisGaythHja24TynVuDSJpBAVFUVGRkaTv2AaY8jIyCAqKsrfoSilmqgm0fqoY8eOpKamsm/fviqPKSk17M3Op2B/OOmNeI7mqKgoOnbs6O8wlFJNVOO9OnoIDw+na9eu1R6TmVfIGQ98w71n9uOKIdUfq5RSwapJFB/VRohTqVDatEuYlFLqqARPUnA+aWkTr3dQSqmj4bOkICKdRGSeiKwTkTUicouXY04QkWwRSXF+fDYYkPtJQZOCUkpVxZd1CsXAbcaY5SISBywTkW+MMWsrHLfAGHOGD+MAtPhIKaVqw2dPCsaY3caY5c7yQWAd0MFX71cTVz8FfVJQSqmqNUidgogkA0OBRV52jxWRFSIyW0T6V/H66SKyVESWVtfstDqhITYraE5QSqmq+TwpiEgs8BEwwxiTU2H3cqCLMWYw8CTwqbdzGGNeMMaMMMaMaNWq1RHF4So+KtHyI6WUqpJPk4KIhGMTwtvGmI8r7jfG5Bhjcp3lL4FwEfHJvJQhWnyklFI18mXrIwFeBtYZYx6t4pi2znGIyCgnngwfxYOIVjQrpVR1fNn66FjgMmCViKQ42/4KdAYwxjwHnA9cLyLFwGFgqvHhAEYhIk1+fCSllDoaPksKxpiF1DCJgTHmKeApX8VQUYhonYJSSlUnaHo0gy1C0pyglFJVC6qkEKrFR0opVa2gSgpafKSUUtULsqSgxUdKKVWd4EoKIaL9FJRSqhrBlRRE5zhWSqnqBE9SyNzOeWYuocV5/o5EKaUCVvAkhd0p3GWep2VBmr8jUUqpgBU8SSGuPQBRh/b6ORCllApcwZMUmrcDoDRHnxSUUqoqwZMUYttQihCaq08KSilVleBJCqHh5EW0IqkojcOFJf6ORimlAlLwJAUgr/VwRoWsY01alr9DUUqpgBRUSSG6z4m0lwNs+W2Vv0NRSqmAFFRJoXmfkwAo2Tzfv4EopVSACqqkQGIP9oe1I3n/9/6ORCmlAlJwJQUR9rYdz+CSNaRnH/J3NEopFXCCKykAsZ0HES0FrF2/1t+hKKVUwAm6pNCu+yAA9mxe6edIlFIq8ARdUojoMJhiQolI/cnfoSilVMAJuqRAVHN2xQ+lf+7PHMgr9Hc0SikVUIIvKQCR/SbTOySVrxYu8ncoSikVUIIyKbQZcQ4AWSmz/ByJUkoFlqBMCiR2J7tZJ7rnLmXnAW2aqpRSLsGZFIDQjsOZELKMNQs+9XcoSikVMII2KcR2G0WIGCb+ej2U6qipSikFQZwUGPF7Dka0BiB7z1Y/B6OUUoEheJNCeDP2nfI4ACmrUvwcjFJKBQafJQUR6SQi80RknYisEZFbvBwjIvKEiGwSkZUiMsxX8XjTtbft3Zy+6jtKS01DvrVSSgUkXz4pFAO3GWP6AmOAG0SkX4VjJgE9nZ/pwLM+jKcSie/ItpbHcMzBr5i3Ib0h31oppQKSz5KCMWa3MWa5s3wQWAd0qHDY2cAbxvoFaCEi7XwVkzcdRp1FB8lgxZo1Dfm2SikVkBqkTkFEkoGhQMUuxB2AnR7rqVROHIjIdBFZKiJL9+3bV6+xhXc/EYCEtW9SWFxar+dWSqnGxudJQURigY+AGcaYnIq7vbykUuG+MeYFY8wIY8yIVq1a1W+Arfuwv+MErij9mIh/tIStC+r3/Eop1Yj4NCmISDg2IbxtjPnYyyGpQCeP9Y7ALl/G5E3SSTeVLZv3pzX02yulVMDwZesjAV4G1hljHq3isM+AaU4rpDFAtjFmt69iqlK38WWLcvgA7N/U4CEopVQg8OWTwrHAZcBJIpLi/EwWketE5DrnmC+BLcAm4EXgDz6Mp1qHTn+mbNm8cRYYbaKqlAo+Yb46sTFmId7rDDyPMcANvoqhLqJHXsL3zSdw6K2LmZSzBNLXQZuKLWiVUqppC94ezV6M792atxJvpoQQWKsD5Smlgo8mhQomjh7E+tJO7N+g03UqpYKPJoUKLhjRifTIZJL2LODAuu9h5xJ/h6SUUg1Gk0IFUeGhDBl+DAAJ750FL5/i54iUUqrhaFLwouUJf+DD+CvcG54fD4U6Q5tSqunTpOBNVHP6T32A20Nus+u7U+CJIbB7pX/jUkopH9OkUIW+7ZozauKlPFI0xW7I3Qvf3uffoJRSysc0KVTjjCFdeCv8fHJCWtgNuenaqU0p1aRpUqhGs4hQrjm+G6MPOaN07FkJqz6EA1v8G5hSSvmIJoUaXDe+O306tyWLOLvh46vhiaHw2xz/BqaUUj6gSaEGYaEhXDOuGxPz/6/8jo+u0qIkpVSTo0mhFk7t14YOXXpwbeEM98aCHC1GUkodna0/QPp6u1x0GIoL/RsPmhRqJSw0hNd/P4o97ScAUBLf2e54cpgdZru0xNY1lJb4MUqlVKPz+pnwzGi7/M+28NQI/8aDJoVai40M4z9TBjMo/wXu6/SKe8ecu2DRc7Y4aeX7/gtQKdX4ZW33dwSaFOqiR+s4xg/uxRtL0zGuUcF/mw1r/2eXi7TXs1KqCh9Ph0c9huOvqqjIGL/WV9YqKYjILSLS3Jkh7WURWS4ip/o6uED0+IVDOKF3K4YXvejeuHORs+D8R+7fBK+dAfkVp6RWSgWtle9BTpp7PT/L+3HvXAj3tWiYmLyo7ZPC740xOcCpQCvgSuAhn0UVwEJChMcuHEJ8y1ZcU3hr+Z1z7oGSYtvzedsC2KjNVpVSVTic6V72fDLY+HX54/IyYO59DTb+Wm2TgmsGtcnAq8aYFdQwq1pT1iI6ggfOHsA3pRUqhYryYP0sCA236x9dpZXPSqnyXMVGnklh7+rKx5WWwpKX4JFusPDRBrvJrG1SWCYic7BJ4WsRiQNKfRdW4DuuZxLvXD2aO4quZlOzwe4d/7sR0pa71/P2N3xwSqmarXgXnjmm4d939wrbFPW7f7i3PXdc5eO2L4QvbnOvf3A5LH/D5+HVdo7mq4AhwBZjzCERScAWIQW1Y3okMXfMVZzy40n8N+ETftfsV8jYBIUH3Qfl7YO4Nv4LUinl3SfX2t8lRe6ne1/Z9qN7ubZztLx+ZuVthw7UTzzVqO2TwlhggzEmS0QuBe4Csn0XVuMxY0JPAP544HekTPoEJLT8Ac8dq0VISgWy2rYa3LvGDnGTl1H393j34rq/xps2/evnPNWobVJ4FjgkIoOBPwPbAd8/xzQCzaPCmX3LOCJCQ7jkjbXs/eMuiIiFZi3dB238xn8BKqXK27YQ1n/hXi86XLvX/fCIHcVgy7zy27NTbUXx5zNg9cd2W2EezP4LpLwDT46ouqURQEL32sceQEmh2BhjgLOBx40xj4NrhDjVt11zPr/pOPIKS7jnf6vJveU3uHGZ+4AGKAdUStXSa6eXv3MvzKvd61wVxJ5FTR9dA//tb+sSl70KHzql6otfsJ1aP70eMjbabe2GeD/vNd9W/Z6THra/z30Jbl0PzdvXLtajUNukcFBE7gQuA74QkVDAx4VwjUvvtnGMTG7J12v28rvnl1IY2RLOeAySx8GGL2D2HfDUKHi4G2Q7bZU3zYXN37lP8r8bYOFj/vkAKjiVFNk74KCcbtZpQFnbJ4USJym8Pw3mP2T7Ia1yRjFIect93KEDsOK9yq8ffgWcdJddTh7n3t6sJdybDX/eCjNWw9lPu/d1ORbuOQCDLoDm7WoX51GqbVK4ECjA9lfYA3QAHvFZVI3Ug+cOZHyvVmxMz+W9pTthxJWQ7LQqWPQs7N8AhzLs4+vMi+Gt8+DN37lP8OtbMPfv/gleBaeUd2wrmB+C8M9ZnMtfxTqFty+A+f+C/Gz45Hp7kV/5ARzc7T5m/oNwYLP38z7cFfatq7y910RItHWQjP+z/R0a4d4fnQAtOsHQS93bknpCSIV6Sh+rVesjY8weEXkbGCkiZwCLjTFaJlJBj9ZxvHblSM599ifu/nQ1CdERnN66X+UDP5ne8MEp5Y2r6CRYhmjxfDI3TgOQdy+B6xa6WwlunGN/ouJhxTuw7vPyLQq9ncubkHAoLXKvx7WFfmfDbb/Z9/rLdjA1tOwPi6z5M9Wz2g5zMQVYDFwATAEWicj5vgyssRIRbj+1NwA3vLOcd3IGww1LoPfpVb+otLR8j8b9G30cpVIO10VJmugwaAe2wKLn7d9XaUn5J3OXvHRY/7ldXvKSe/tXf7G/vSUEgG/vt78HTXVvG3qZe7nbCeWPF7E/ruTTrIV9OvBm6jswdab3fT5W22/C34CRxpjLjTHTgFHA3dW9QEReEZF0EfHSVQ9E5AQRyRaRFOfnnrqFHriO6ZHErJuOIyYilL9+upobvsml+NyX7B2CN7NugeJ89/pTI3QCH9Uw/JUUVrwLB/fUfNz6L+xIxEfqgyth9p8hZxcUVHFxBwiPhuKC8p3FamPMDXDm4+71Mx+3TwBnPQlnOvWDPU6B81/x/vqq9Dkd+kyu22vqSW2/CSHGmHSP9YxavPY1YGINxywwxgxxfu6vZSyNwoAO8Sz+m+2k8sXK3Xy+9oC9Q2jeofLBy9+ALd+X31aY65vA8jLg/sTynWlU8CpLCg00as3hTMjaaTuOvVWLwoZ3L4afnrS9gDO3wZKXq+/AtWE27FziXncVEe1ZWf3f1KfXw3uXVr0fYOyNlbfFtYHwKPd6SKh9Ahg2DeI72grkSz+CAedVf+4AUtsezV+JyNeA63nmQuDL6l5gjPlBRJKPPLTGLyYyjK9nHM9pj/3AH99bQYgIZ89YDSUFsGcVvDPFPf7JzAvLv/hQBkTGwbpZ9gt94l9rfsPiAltZPfxKCKkiZ6cthdJi+PExSD726D6gavwa+knhX8nu5Yw6FJM+f7x7ect8uPBN78fNdIpy7nX61jbvaP/W9q6GlsneX+NS09hCo66BnhOgKN/99+pZUdxE1OqbYIy5HXgBGAQMBl4wxvylHt5/rIisEJHZIlJlrwwRmS4iS0Vk6b59++rhbRtO77ZxPHfpMABueTeFB7/egAmLgk6j4C/bbDO0DsMrv/DD39vf710C3/+rdsVJCx+DL251N5PzxvXHX1MFl2qaSksqFKO4vlc+elLY8r29sfHGs8jUm6qGnt+7BuY9aJvTLn+j+ialrmakS1+t/SRYrZ1L0cXvw4Vvu7c3S7D1BL0nwmCnn0Ok011r/B1w2oO1O3+Aq/XtgTHmI2PMrcaYPxpjPqmH914OdDHGDAaeBD6t5r1fMMaMMMaMaNWqVT28dcOaOKAdf55oK5+f/34LJ//new4XOo+10QlwzXfQokv5F6Utg+/+6V6vqfx124+QtcMuVzsIn/PHr0mhvKIaLlANZeM37v9HlxdOgCe93Dgcidl/hgc7uj+vr54UDu61v984y97YVOXeFpBRRdPO96d5335gM3z/EHxyHXx2k53Gcv/Gyv+Hactgs9MxLCfNjjQKcOVX1cfu6pzWLAH6ngHnPAstu7oTANi6g8n/dlcyn3gnjP1D9edtJKr9JojIQRHJ8fJzUESOagYZY0yOMSbXWf4SCBeRpKM5ZyC7fnx3NvxjIj1ax7Jlfx6nPPo9hcUeF+azn7K/T/+Pe9sPD7uXH+1jO8Ts32hbK3nKz4bXJrs70JRUM/m3q4lcfSWFn5+GvWvr51z+svxN+GcbW9btb2+fX3nEzF3OQIv1YYNzQdw01/52jctVn0khZSb8pxfsSqnFwcbOdb57pVPR+yd49XT7ZFxxOImKNnkMH/PWefDvXu7158bBiyd5f53nxf3Sj93Ld+yAW1a4/35czUGHXAy3pJSvdwmLsMVJobUtgW88qv1ExhifDWUhIm2BvcYYIyKjsAnqCEaaahxEhMiwUL6ecTx/+2QV7y7ZSa+7ZvPqFSM5rmcS4V2Pt+WgxYXw2xxbWbXpWzjkcdc/a4ZtT37cH6HTaHv3c2CrbangqbS46kBc7dHr0rpp/ZeQ8jZMfbv89qJ8+PqvEBEHf02t/fkCzS/P2N8Hd9vOQ/7iSvb5RzjW5OFMe46YxKqPaTcYclJtPVXfM9xFL4V59ruU0NUO3dB5NIy8+sjicJXNu0YhBVt8026w9+MBnh9ni1HTnOFhatPL2PPfqeLcxntWVv26yDibAELCISLavT0q3v6c8yx8/zC06lNzDE2Qz9KciMwETgCSRCQV+DvO0BjGmOeA84HrRaQYOAxMdcZXatJCQ4QHzx1IYUkpHy9P48rXljCiS0tevnwk8dHh9g7kEqfss7TU3p1s/ta2tXb9sS38b/mT/vxU+fXifHuX1rqfPZ8n13AGrieFrJ3w0slwxZeQ1MOux3eE3SmQ0M3+kbx7kXPewvLncyWsqtpxN5TUZbD0FdsMsKoK9upkOwmtpjLuuirItReueC8tzrzx9v51mY/j373sXe5pD9qe9O0GVT7G1Rpn3wbnPQvs70XP2p87dtg6qVXv1y4ppLwDC/5j++Jg7CBwu5z5RPatdx83a0bN50rzGC+sqgHkWnSp2+T2f/gF4jvBrD+669qimtvvdVXaD4GL3qn9ezQxPksKxpiLatj/FPBUdcc0VSLCo1OGMKFvGx6cvZ6l2zM57l/f8fQlwzi+l0ediesC1+MU+2TwYMfavcGqD+wfKsCEB+DYm937XHdg2xbAqg8heyfk7rWDefU/F146CXqeZqcE7HIsXOnRyKwgB8I8SvjyAqTS/63f2YvvhPsg5ghKIAucktD6nFP7gda2lRm4W8JUJz8HctPLb8vcDo97ubBXxVXs8fWdld+3uNB20HL1YD6w2Q7i5ipGctn+k3t575ryo3Lm7bfFTJ4drr64zT59PjGkbhfrmrx/ufftgy4sX6xa0U3LbXGUS+u+9vd5L9ongOwd5UcwBrjuRwhpesVAR0r/Jfxo0sB2TBrYjsVbDzDl+Z+Z9spi7jq9L1eP61b54Mg4O0ritoW2fPn7aqbI9qyo/OZuu95ukK0UK/IYEfKjq6DNAPe66w7PNUfs9gp9GfKzy1906zqrXFG+vVONiKnb62rier482jv96jo31cauFPuUFZPkTgi19d8BUFAhedQ2Iaz/0hb7eGOMbee/61dY41F+nrnDNtWsaLNHOf6zx5RPLI90h/AY+Nsuu/7KJHdxZH0mBIDUxeXXB15gb3aGXupOCn/4xT7p7FvvLqpK9BiG+tb15c8RGmaffitqO6DytiDWRPu2Ny6juiZw1+n2juYfX6yj112zOZDnpbK4eTs7WuKJd0Js2+pP6rl/yYu2lcYHl1cuq/WcGzYnrfpzVnyk93xSKC2teTKh546F//PF0L9OVqjtEMieSjzGpqlLUtg41zaHXOcMj1BaCi+Mh1cnVW4I4Pkem+fBvfGVW5NVTAgpXooviit8J/L222aW714Ez4zxHueBLfbGwDMheHs/lxXvll93FTe6vjeumwpjYMdP1NnEh7y37b/kQ+/Hz1hlE9N5L9nfLT1a6bXua4t6elSYyeyid+0NUAONKtrU6JNCgLh6XDdaRkdw2wcrKCwuZdgDtmXFkxcN5czBXi6kV8+F1CW2v8N/vXTxuGimrYOY79F2esOX9sebn59yj+BYlYoVoJ5J4aWT7SN4r1PtHfPUt+2F5H83wsl3245DrhY0e1Z7vzs7nGnvegtzof/vILZ19fG4uKqinh5Vu6Kacp/B42nH24Vyzae2uKTr8eW3v+3RQ/XaBe4iif2/Va5jKcy1+4sOw5vn2G1f3Wnvak9tZXHQAAAdA0lEQVSqYgiHT6+vvK3gIMz9D4y5znbKeqQWk7O46g5qq+K/waEMWxnr2ZLohRNqbnE2aCqsfLfy9pHX2CKgrB02ibokdIO79sE/PIpP7zngfYTQMx+HqBbudde//YQH7O/ek+yPOiKaFALIecM70q5FFEu2ZvLfuXacpJtm/sqKnVmcO6wj/do3dx/copO7pcwdO+zYLd/cY9t8tx1oW3pkO00sO42GnYvKv1lEXOWLV8ZG6H5S+dEfPed3qJgUPMvAXUVPrsf+9PX2fKs/tEUpF3qMN//csd4v3h9c6W6GuGE2TKuy60rVDu6xo1G6GGOHhh48FRJ72KeJyFjnPb6y9Sku2V6elD5wyrarSzbbf4JWHs0hK9ZNFObZC9ey19zbXHfuiT0gsjm1snU+/PK0LS4568majy88BOleLt6R8eUv/lHxVbd42vSNraT1tOvXyseNuhYWP2+Xz3jMXvi3/gAHnaImCYGbU2wRTnSC/fc472W7ffuPth9ASIh97awZMOmRqoeMHn5F+fWQ0LrfDKgqaVIIMMd0T+KY7klcMqYz6TkF3Pp+Ci8t3MpLC7fy690TaBnj5dHb1ZJiYoUelX3OhFtWQovO8OPj7rkaps60dQxz7oI1Ffohjr6+fFLwnN9hycu2iCiuHXQea++yQyO9l59v/d5WlIL35q/7frMX0p2LbRHWiN+Xb5eet9++ruKYPKUltkI1awe06u1s9Dh/6lLb1NIlazss+Des+wwGnA/z/w+6jofR17lbVbksexUGTYEux3iP+3CmvbCPuaF8Ut21vHw7/4IKSWHeg3DGf+1dd0WezTa98XwfVy/3zd+6lz216Fy+Punzm205fEUtu7ibbN6ZaucP2PGzXR97o32iWfqyXa+YELwZNR0mP2yTQnSSnUcE7IxiWTvstojo8rOGicBAZ+yjAee6t4+40l70G2osJlWJJoUAlRQbSVJsJB9dfwwD7v0aY2DoA99w88k9uXVCr5pPAPbOy1UGe9wM+0f5y7O2DDYswk7xFx5t7yZdd38dhnk5T5jt+7Btgf0Be4ebsQnaD3M/JbiERtiesy7bf6zckenpkTDuNncrqe4nl9+/dxU81MU2z+08xiaYTXNtj9/fZttjrpoLnUaWv3i/dwmc8Fc4fAD6neNuxpuz2yYEsAlrq8cAhJHxcNwtdijkVyfB37PsRcnz4l5aYhPC3HttAvC8i01dUj4pbF1Q/rOkvGUr7z2LPGrjzCfshd2bnb/Y36fcB5lbbWyt+5VPCt4SAtjxe1xJITIOjr3FJoVh0+C0f9q5hr3pf677CWfiv2wy//VNW7kOcPW37mWw37cjmT5SE4JfaVIIcDGRYXx763hufOdX1u7O4YlvN/LMvE2c2Kc108Z2YXiXlkRH1PK/cdAU++MSGgbnOB239m+0j/uerYsu+cj2RRg81bYh9+wh6qof8GzeN/Eh6DHBjjbpOfPU4czy5ccuCzx6b3traVOQDa+cBlfOthfril4+xc5mRYU7etfFf9Fz7m3V9aWIjLUXVJePrrbFSts8Lu5Z20GcRLBtoS3f7zXJXlRXvW8rdF1t6L/yMixY3r66NeHtNRF6nWaLlio+eXjqeaqdDxhsZ6vfKgzhEJ0I4/9i6zpSZtqK4oFT7BNae2fO4N6TbL2I6wI+/s/2KSxnl/1eNG8Pwy6DZa+7z9t1nLtfQfuh9nfHEbX/fCpgSWPrLzZixAizdOlSf4fhF9mHi7h55q+Eh4Ywd527LPyeM/pxyZjORIbV07R9s261TwIVx3L56k7bBNBVtABw8j32Apm2HC7/zG579xJYPwuOudleWFZX0bKkoi7HwfaF9fMZ6qJFF7jgNXjxxLq9bsL9dij0j66y62NusGX+R2vgBba1DdiL8r717mGdW/W1idqVYG7bYFtAffknuPaH8qOJgvupB2yR3LaF0P+cI4srZ7cdbsX1vmD7uoz5w5F1GlQNSkSWGWNqzNz6P9mIxDcL5/Xfj+Kly0fw18l9aB9vx3G/f9Zaznn6Jx6ds4FN6fXQu/iMR70P7jXxQbvvonfh+NvhTxttEdAp97oTArg7DHU5Fs590entWsHNKXDxB3CBc/fZ81S48ovyY9af8Vjl1wFcPstOZDLEY6C1jiPdy33PtHUGLv3Othf+rl6eVsAOgFZVS6fwaO/bwTYRdT1htOxqO8+53FZFq59LPrL1J2Xn99Jn49wX3ctJPe3niXHim/Yp3L7Jvd4swZ7v9i3uYSRi20KHEXDZJ+WLYmKSjjwhgG3iOf17GHGVff+4tnDMjZoQmhh9Umjk3l60nblr97IiNbusb8NtE3oxfXw3SkpN7YuW6lNxgW362u8c90XpXo9hBUZfB5P+5V7Py7CV5a7BxX58wratv/IrWDETlr9umxvuW2+bd17wuvu8xYW2aKTtAPfYP9EJdv9rZ9gioIs/sE1ljbF1J7t+hR/+7W4ZM2yaTUCfXGebzro6R/WaaMvsI6Lte7zlNEPtOMq2spo6086O9dvXkDzOHrfmE9tc84Q77XwZ7QbZC+iWebZl2A2LAWP7K7x9nm2l036YnQcjY6NNAtd5eVo6nAk7Ftlhm8EOzZG6xDbdrXhcaET9dxBUjV5tnxQ0KTQRxhheXriVf3yxrtK+5XdPIMFbq6WGtGG2vfCFhsPk/1Q/aBvYoo6YJHcl8pFUPv78jB3y4cZldlwnTzm7bSyZ22wTXs8J0nPTvT853Bvv7tFbkGsvvEdaKWqM/fcYcJ57ULbCQ7YC2w+TtaumT5NCkNqVdZjbP1zB9oxDpGa6ey/Puuk4EmIiaBcfhQRL6w5jbL+F+urZmpdhk0BVk60rFcA0KShKSw3nP/cTy3eUH57ihhO7c+WxXUmK1TtSpYKFJgVVZlP6QaY8/0ul8ZRGJSdw/Ynd2bIvj7jIMKaM9ONcAkopn9KkoMrJLyqh1Bgycgu57/O1rErL4lBhCQfz3RPyfPPH4+nZxmfzKiml/EiTgqrRocJiHpi1lpmL3dNQnta/DQ+cM4DWcVF+jEwpVd80KahaM8awIjWbm2YuZ+cBWzmdEBPBw+cNYkRyS1an5XBsj8TgqaBWqgnSpKCOyEsLtnht1nru0A6cO6wjx3RPJCREk4NSjY0mBXXEjDFkHy7iileXkLKz8ly5fzihO9ef0J092fm0iI6gVZy2YlIq0GlSUPWipNTw0fJUnpm3iZ2Zhykprfx9eeea0Yzpqk8QSgUyTQqq3pWWGmYu2cHfPlntdf/E/m258aQeDOgQ73W/Usp/NCkon/l1RyYdWjZjzpq93PVp5QQxtHML/ja5LyOSteevUoFCk4LyOWMMBcWl5BYUs+9gAf/35ToWbHTPeRwRGkJhSSmTB7blgbMHkKg9qJXyG00KqsEZY8grLOGzlF3MXLyDVWnueXMTY2yF9IxTepJ1qIgpIzppHYRSDUiTggoIOzIOsS/XPkUs255Zbt+tE3px88k9/RSZUsFFk4IKKMYYPk1J438pu5i/wT0t5aCO8YgI08Z04aQ+rYmLCiMsVCdtUaq+aVJQAevNn7dx9//WIGJnk8s6VFTpmBtO7M7tp/Vp+OCUaqL8nhRE5BXgDCDdGDPAy34BHgcmA4eAK4wxy2s6ryaFpqOwuJTQECEjr4CfN2ewZNsB3vplR9n+sBCha1IMM07pxYR+bYgI0ycIpY5UICSF44Fc4I0qksJk4CZsUhgNPG6MGV3TeTUpNF3GGK58bUm54iWXNs0juWZcN/IKSjhnaHu6JOp0k0rVhd+TghNEMjCriqTwPDDfGDPTWd8AnGCM2V3dOTUpNH05+UXc+781XDa2C6t35ZCVV8g7i3ewOzsfgOiIUK4f350Dhwr52+S+hIhoSyalalDbpOCHWd3LdAB2eqynOtuqTQqq6WseFc6jFw4BYGjnlgBceVxXPl6eSo/WsUx7eTH/+eY3AF79cRsA08Z2YdrYLvRorfNBKHU0/JkUvN3aeX1sEZHpwHSAzp07+zImFaBiI8OYNjYZgMenDi3rVX3f52sBeOPn7XywNJVrx3fjjEHt6d4qhlIDofoEoVSdaPGRatSMMWxMz2V/bgF3f7qazfvyAJsMEmMiGNAhnoEd4rlsbBedk1oFtcZQfPQZcKOIvIutaM6uKSEoVZGI0KtNHL3axPHERUO54tUlDO4YT25BMct3ZPHd+nS+W5/OT5v3c8noLkRHhNK/Qzzt46N00iClvPBZUhCRmcAJQJKIpAJ/B8IBjDHPAV9iWx5twjZJvdJXsajg0L99PEv+dkq5bdsz8rjhneUs2ZbJkm3uHtURYSEs/POJRIaH0jwqTBOEUg7tvKaCQvahItIP5vPs/M18/Gta2XYRGNstkTsm9WFQxxZ+jFAp3wqIJqm+oElBHa3sw0X884u1fLc+ncxDRWUTByXFRvCPcwYwMjkBA7SMjtCKatVkaFJQqhaKSkr59Nc0/jPnN/bk5Ffaf86Q9kwc0I7CklLOGtzeDxEqVT80KShVR1v357EqLZv569PLFTF5uuKYZG4+uScH8grp3ipG6yJUo6FJQamjkFtQzIHcQrbsz+WKV5dUedzcW8fTpnkkcVHhDRidUnWnSUGpepKek48BVqZm8+DsdWxx+kJ46tM2jneuGUNCTASlpUaH3VABR5OCUj5yqLCYe/63hjlr9tC6eRSb0nMBiIsMIyIshFZxkTxy/mD6t2+uyUEFDE0KSjWQ0lLDsh2ZPPzV+nJ9IQAuHdOZU/u1ZVTXBMJCRCcQUn6jSUGpBlZUUkpq5mE27j3I9DeXARAeKhSV2L+xzgnRXHN8N8b1SGLr/jxGdk0gNtKfgwqoYKJJQSk/WrY9k9ZxkUSGh/DGT9tZsyubHzdnUFhcWu6468Z3545JOsOc8j1NCkoFmMy8QtbtyeHj5Wl8uCy1bHtMRCi928bx4LmD6N1Wh/5WvqFJQakAtio1m5Sdmfy4KYOv1uwB7PSjxaWGfu2a075FFFeP60ZcVBi928RpXYQ6apoUlGok5qzZQ2JsJPfPWsuKnVmV9ndo0Yyk2AhGJCdw56Q+miDUEdGkoFQjU1Jq2JuTT35RCTGRYUx/cxnNo8JYvj2TvMKScsd+NWMcfdo291OkqjHSpKBUE7Fk2wE27s2l1Bju+nQ1YOshXIni9tN6M/34boTrE4SqhiYFpZqgnPwivl23l2fmbWaj02kObI/qkckJNG8WRtvmUUwd1VmThCpHk4JSTVxJqSEt8zDfrd/L499uJPtwEaUef8692sTyzCXDiIsKp1VspPauDnKaFJQKIvlFJRSXGg4VFnP/52uZtbL8zLY9WscyuGMLpo3twuBOOplQMNKkoFSQKiwuZeGmfYSFhPDw1+sBWJ2WA9iZ5u4/ewCRoSG0iA6nc2K0VlgHidomBe1jr1QTExEWwkl92gBwfK9WgC1qmrc+nZcWbuFup7LaZVzPJDbuzeXU/m346+S+RIWHNnjMKnDok4JSQaS01PDFqt3szcnn+9/2sWDjfgAiw0IocIbg6JwQzbXjuxERGsLpg9oRHaH3jk2BFh8ppWolI7eAEBEembOBdxbtKLdvbLdEzhjcjlIDl43p4qcIVX3QpKCUqhNjDN+s3UuvNnH89ZNVhIZI2ZOES2JMBL8b2oE7JvUhLeswXRJj/BStqitNCkqpo/bN2r0YY/hufTrvLtlZaf/4Xq246riuZXUXKnBpUlBK1asdGYf4afN+3vxlO2t25ZTblxQbycjkllw2pgtREaF0bNGMVnGRiGjfiEChSUEp5RMlpYbDRSXERISyYe9B/vDWcrbsrzxvNcCNJ/bg8mOSEbGJQ/mPJgWlVINZviOT/KISPl+xm5mLd1TaHx4q3HZqb4Z2akFBcSnH9UjSHtYNTJOCUsqvVqdl89Yv28k+XMTqXdnsPHC4bN+4nkkM79KS84Z1pFNCtB+jDB6aFJRSAcMYw5b9eXy4LJX3luzkQF5h2b4+beO458x+ZB0qIjYyjBHJLbVvhA8ERFIQkYnA40Ao8JIx5qEK+68AHgHSnE1PGWNequ6cmhSUatxyC4o568mFHCwoZt/BAq/HPHL+IIZ2bkn3VjFaWV1P/J4URCQU+A2YAKQCS4CLjDFrPY65AhhhjLmxtufVpKBU42eModTYuSK6JsXwr6/Wszotm9/25pY7Lik2kpbR4dx8ck9O7d+GyDAdguNIBcLYR6OATcaYLU5A7wJnA2urfZVSqskTEUIFxnRLBODRKUMAO5jfzsxDPD1vE7NW7GZ/bgH7cwu4aeavtIuPYniXlvRvH09BcQlrd+Xw7KXDCdUK63rlyyeF84GJxpirnfXLgNGeTwXOk8KDwD7sU8UfjTGVesiIyHRgOkDnzp2Hb9++3ScxK6UCy+q0bJZtzyQ2Mox/z9nA7uz8cvuvG9+dFtHhnNSnNd1bxWqCqEYgFB9dAJxWISmMMsbc5HFMIpBrjCkQkeuAKcaYk6o7rxYfKRWccvKL2HewgMOFJbyycCsf/5pW6Zh/XzCYSQPaEhOpFdUVBUJSGAvca4w5zVm/E8AY82AVx4cCB4wx8dWdV5OCUgpg496D/LL1ACt3ZvHBstSy7S2jwwkNCaFXm1iuPLYrCTHh9G7bnNggTxSBUKewBOgpIl2xrYumAhd7HiAi7YwxrimizgLW+TAepVQT0rNNHD3bxMGYLkwbm0ypMazelc0XK3fz0+YM9ucW8NPmjLLjJw1oy4PnDqRFdIQfow58vm6SOhl4DNsk9RVjzD9F5H5gqTHmMxF5EJsMioEDwPXGmPXVnVOfFJRS1THGMH/DPhJjI1izK4dv1u7lu/XpZfvjIsPo0LIZZw5uz3E9kujVJo5mEU2/VZPfi498RZOCUqquSksNLy/cyj+/rFwYER0RyrieSYzr2YpLRndusv0iNCkopVQFmXmFiEBhSSmzVuzm/lnlW8if0rc1zZze1BeO6MRxPZP8EaZPaFJQSqkaGGN4a9EOosJCWL4ji5mLdxAbGUZuQTEAXRKjmTKiE+8s2sHFoztzzbhuRISF+DnqI6NJQSml6uhwYQmRYSEcKirhD28v54ff9nk97tR+bbhwZCdO7N260Yz2qklBKaWOgjGG13/aRtbhIoZ1bsn2jDye+34LaVmHyx138ejOjOmWyOkD2wV05zlNCkopVc+MMWQdKuLjX9P4LCWNFanZ5fbHRoZx00k9OKF3a1rHRdIyJnCav2pSUEopH1u2/QDNwsOY/MSCKo9JToymY8tourWKITkxhotHdyYqvOGbwGpSUEqpBpKaeQiA/KISiksNP23K4KHZ6yksKa10bEJMBKO7JvDERUMJD224SmtNCkop5Ud5BcU8/8MWlmw9wHE9k/hlSwYLNu4vd8zYbolcMKIjPVrHEh0RRo/WsT6LR5OCUkoFmAN5hXy7bi8vL9zK+j0HK+0/fWA7RndLIC4qjF5t4khOjKm3wf00KSilVADbsOcgy7ZnsnV/LjsPHGZFahb5RSVkHioqd9zYbol0SYxm2thkOiY0o3lU+BG9XyAMiKeUUqoKvdvG0bttXLlthcWlbN6Xy7b9eSzbnkn6wQJmr97Nz1syeHfJTq4Z15W/nd7Pp3FpUlBKqQARERZC33bN6duuOZMGtgPAmCGsSsvmw2WpXDSqs89j0KSglFIBTEQY1LEFgzq2aJD3a5yDeCillPIJTQpKKaXKaFJQSilVRpOCUkqpMpoUlFJKldGkoJRSqowmBaWUUmU0KSillCrT6MY+EpF9wPYjfHkSsL/GowKHxus7jSlWaFzxNqZYoXHFezSxdjHGtKrpoEaXFI6GiCytzYBQgULj9Z3GFCs0rngbU6zQuOJtiFi1+EgppVQZTQpKKaXKBFtSeMHfAdSRxus7jSlWaFzxNqZYoXHF6/NYg6pOQSmlVPWC7UlBKaVUNTQpKKWUKhM0SUFEJorIBhHZJCJ3+DseABF5RUTSRWS1x7YEEflGRDY6v1s620VEnnDiXykiwxo41k4iMk9E1onIGhG5JVDjFZEoEVksIiucWO9ztncVkUVOrO+JSISzPdJZ3+TsT26oWCvEHSoiv4rIrECPV0S2icgqEUkRkaXOtoD7Ljjv30JEPhSR9c73d2wAx9rb+Td1/eSIyIwGjdcY0+R/gFBgM9ANiABWAP0CIK7jgWHAao9tDwN3OMt3AP9ylicDswEBxgCLGjjWdsAwZzkO+A3oF4jxOu8Z6yyHA4ucGN4HpjrbnwOud5b/ADznLE8F3vPT9+FW4B1glrMesPEC24CkCtsC7rvgvP/rwNXOcgTQIlBjrRB3KLAH6NKQ8frlw/rhH3cs8LXH+p3Anf6Oy4kluUJS2AC0c5bbARuc5eeBi7wd56e4/wdMCPR4gWhgOTAa2xM0rOJ3AvgaGOsshznHSQPH2RH4FjgJmOX8kQdyvN6SQsB9F4DmwNaK/z6BGKuX2E8FfmzoeIOl+KgDsNNjPdXZFojaGGN2Azi/WzvbA+YzOMUVQ7F34AEZr1MUkwKkA99gnxSzjDHFXuIpi9XZnw0kNlSsjseAPwOlznoigR2vAeaIyDIRme5sC8TvQjdgH/CqUzT3kojEBGisFU0FZjrLDRZvsCQF8bKtsbXFDYjPICKxwEfADGNMTnWHetnWYPEaY0qMMUOwd+CjgL7VxOPXWEXkDCDdGLPMc7OXQwMiXsexxphhwCTgBhE5vppj/RlvGLaI9lljzFAgD1v8UpVA+LfFqT86C/igpkO9bDuqeIMlKaQCnTzWOwK7/BRLTfaKSDsA53e6s93vn0FEwrEJ4W1jzMfO5oCNF8AYkwXMx5a3thCRMC/xlMXq7I8HDjRgmMcCZ4nINuBdbBHSYwEcL8aYXc7vdOATbOINxO9CKpBqjFnkrH+ITRKBGKunScByY8xeZ73B4g2WpLAE6Om05ojAPpZ95ueYqvIZcLmzfDm27N61fZrT2mAMkO16nGwIIiLAy8A6Y8yjgRyviLQSkRbOcjPgFGAdMA84v4pYXZ/hfOA74xTQNgRjzJ3GmI7GmGTsd/M7Y8wlgRqviMSISJxrGVv2vZoA/C4YY/YAO0Wkt7PpZGBtIMZawUW4i45ccTVMvP6oQPFTpc1kbIuZzcDf/B2PE9NMYDdQhM34V2HLhr8FNjq/E5xjBXjaiX8VMKKBYz0O+1i6EkhxfiYHYrzAIOBXJ9bVwD3O9m7AYmAT9rE80tke5axvcvZ38+N34gTcrY8CMl4nrhXOzxrX31Mgfhec9x8CLHW+D58CLQM1VieGaCADiPfY1mDx6jAXSimlygRL8ZFSSqla0KSglFKqjCYFpZRSZTQpKKWUKqNJQSmlVBlNCko1IBE5QZxRUJUKRJoUlFJKldGkoJQXInKp2DkZUkTkeWeAvVwR+Y+ILBeRb0WklXPsEBH5xRnP/hOPse57iMhcsfM6LBeR7s7pYz3G93/b6S2uVEDQpKBUBSLSF7gQO+jbEKAEuASIwY5HMwz4Hvi785I3gL8YYwZhe5W6tr8NPG2MGQwcg+29DnaE2RnY+Si6Ycc+UioghNV8iFJB52RgOLDEuYlvhh2ArBR4zznmLeBjEYkHWhhjvne2vw584IwN1MEY8wmAMSYfwDnfYmNMqrOegp1TY6HvP5ZSNdOkoFRlArxujLmz3EaRuyscV90YMdUVCRV4LJegf4cqgGjxkVKVfQucLyKtoWzu4S7YvxfXqKUXAwuNMdlApoiMc7ZfBnxv7FwTqSJyjnOOSBGJbtBPodQR0DsUpSowxqwVkbuwM4uFYEexvQE7QUt/EVmGne3sQucllwPPORf9LcCVzvbLgOdF5H7nHBc04MdQ6ojoKKlK1ZKI5BpjYv0dh1K+pMVHSimlyuiTglJKqTL6pKCUUqqMJgWllFJlNCkopZQqo0lBKaVUGU0KSimlyvw/6qHxny043QMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved trained model at C:\\Users\\lenovo\\Desktop\\Emotion_Regognition\\saved_models\\Emotion_Voice_Detection_Model.h5 \n"
     ]
    }
   ],
   "source": [
    "model_name = 'Emotion_Voice_Detection_Model.h5'\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n",
      "accuracy: 46.39%\n"
     ]
    }
   ],
   "source": [
    "# loading json and creating model\n",
    "from keras.models import model_from_json\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"saved_models/Emotion_Voice_Detection_Model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    " \n",
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(x_testcnn, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "194/194 [==============================] - 0s 838us/step\n"
     ]
    }
   ],
   "source": [
    "preds = loaded_model.predict(x_testcnn, \n",
    "                         batch_size=32, \n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds1=preds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = preds1.astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = (lb.inverse_transform((abc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predictedvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  predictedvalues\n",
       "0      male_angry\n",
       "1    female_happy\n",
       "2      female_sad\n",
       "3  female_fearful\n",
       "4  female_fearful\n",
       "5      male_angry\n",
       "6    female_happy\n",
       "7    female_angry\n",
       "8      female_sad\n",
       "9      male_happy"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preddf = pd.DataFrame({'predictedvalues': predictions})\n",
    "preddf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual=y_test.argmax(axis=1)\n",
    "abc123 = actual.astype(int).flatten()\n",
    "actualvalues = (lb.inverse_transform((abc123)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actualvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     actualvalues\n",
       "0      male_happy\n",
       "1  female_fearful\n",
       "2  female_fearful\n",
       "3  female_fearful\n",
       "4  female_fearful\n",
       "5      male_angry\n",
       "6      female_sad\n",
       "7    female_angry\n",
       "8      female_sad\n",
       "9        male_sad"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actualdf = pd.DataFrame({'actualvalues': actualvalues})\n",
    "actualdf[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "finaldf = actualdf.join(preddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actualvalues</th>\n",
       "      <th>predictedvalues</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>male_happy</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>female_fearful</td>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>female_fearful</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>female_fearful</td>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>female_fearful</td>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>female_sad</td>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>female_angry</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>female_sad</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>female_sad</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>male_happy</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>male_happy</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>male_calm</td>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>male_calm</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>male_fearful</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>female_angry</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>female_happy</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>female_happy</td>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>female_calm</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>female_sad</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>female_fearful</td>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>female_fearful</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>male_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>male_fearful</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>female_sad</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>female_happy</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>female_calm</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>female_sad</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>female_happy</td>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>male_calm</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>female_sad</td>\n",
       "      <td>female_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>female_calm</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>female_sad</td>\n",
       "      <td>female_fearful</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>male_calm</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>male_happy</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>male_calm</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>female_sad</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>female_calm</td>\n",
       "      <td>female_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>female_angry</td>\n",
       "      <td>female_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>male_fearful</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>male_calm</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>male_sad</td>\n",
       "      <td>male_calm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>female_fearful</td>\n",
       "      <td>female_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>female_happy</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>male_fearful</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>male_calm</td>\n",
       "      <td>male_sad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>male_angry</td>\n",
       "      <td>male_angry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>male_happy</td>\n",
       "      <td>male_happy</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       actualvalues predictedvalues\n",
       "0        male_happy      male_angry\n",
       "1    female_fearful    female_happy\n",
       "2    female_fearful      female_sad\n",
       "3    female_fearful  female_fearful\n",
       "4    female_fearful  female_fearful\n",
       "5        male_angry      male_angry\n",
       "6        female_sad    female_happy\n",
       "7      female_angry    female_angry\n",
       "8        female_sad      female_sad\n",
       "9          male_sad      male_happy\n",
       "10       female_sad     female_calm\n",
       "11       male_happy      male_happy\n",
       "12       male_happy      male_angry\n",
       "13        male_calm    female_happy\n",
       "14        male_calm    male_fearful\n",
       "15     male_fearful    female_angry\n",
       "16     female_angry    female_angry\n",
       "17         male_sad        male_sad\n",
       "18       male_angry      male_angry\n",
       "19         male_sad      male_happy\n",
       "20     female_happy    female_angry\n",
       "21     female_happy    female_happy\n",
       "22      female_calm     female_calm\n",
       "23       female_sad      female_sad\n",
       "24       male_angry      male_angry\n",
       "25   female_fearful  female_fearful\n",
       "26       male_angry      male_angry\n",
       "27       male_angry      male_angry\n",
       "28   female_fearful      female_sad\n",
       "29         male_sad    male_fearful\n",
       "..              ...             ...\n",
       "110    male_fearful      male_angry\n",
       "111        male_sad        male_sad\n",
       "112      female_sad      female_sad\n",
       "113        male_sad       male_calm\n",
       "114    female_happy    female_angry\n",
       "115     female_calm     female_calm\n",
       "116      female_sad      female_sad\n",
       "117      male_angry      male_angry\n",
       "118    female_happy    female_happy\n",
       "119       male_calm       male_calm\n",
       "120      female_sad      female_sad\n",
       "121      male_angry       male_calm\n",
       "122        male_sad       male_calm\n",
       "123     female_calm     female_calm\n",
       "124      female_sad  female_fearful\n",
       "125       male_calm        male_sad\n",
       "126      male_happy      male_happy\n",
       "127       male_calm       male_calm\n",
       "128      female_sad    female_angry\n",
       "129     female_calm     female_calm\n",
       "130    female_angry    female_angry\n",
       "131    male_fearful      male_happy\n",
       "132       male_calm       male_calm\n",
       "133        male_sad       male_calm\n",
       "134  female_fearful    female_happy\n",
       "135    female_happy      male_happy\n",
       "136    male_fearful      male_angry\n",
       "137       male_calm        male_sad\n",
       "138      male_angry      male_angry\n",
       "139      male_happy      male_happy\n",
       "\n",
       "[140 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finaldf[0:140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "finaldf.to_csv('Predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#livedf= pd.DataFrame(columns=['feature'])\n",
    "X, sample_rate = librosa.load('data/03-01-03-02-02-02-01.wav', res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\n",
    "sample_rate = np.array(sample_rate)\n",
    "mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\n",
    "featurelive = mfccs\n",
    "livedf2 = featurelive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "livedf2= pd.DataFrame(data=livedf2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "livedf2 = livedf2.stack().to_frame().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>206</th>\n",
       "      <th>207</th>\n",
       "      <th>208</th>\n",
       "      <th>209</th>\n",
       "      <th>210</th>\n",
       "      <th>211</th>\n",
       "      <th>212</th>\n",
       "      <th>213</th>\n",
       "      <th>214</th>\n",
       "      <th>215</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>...</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-57.414993</td>\n",
       "      <td>-57.414993</td>\n",
       "      <td>-57.414993</td>\n",
       "      <td>-57.414993</td>\n",
       "      <td>-57.414993</td>\n",
       "      <td>-57.414993</td>\n",
       "      <td>-57.414993</td>\n",
       "      <td>-57.414993</td>\n",
       "      <td>-57.414993</td>\n",
       "      <td>-57.414993</td>\n",
       "      <td>...</td>\n",
       "      <td>-27.840111</td>\n",
       "      <td>-27.892956</td>\n",
       "      <td>-29.425554</td>\n",
       "      <td>-31.255728</td>\n",
       "      <td>-34.011845</td>\n",
       "      <td>-34.322586</td>\n",
       "      <td>-33.564934</td>\n",
       "      <td>-36.074425</td>\n",
       "      <td>-37.658695</td>\n",
       "      <td>-38.918003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  216 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0          1          2          3          4          5    \\\n",
       "           0          0          0          0          0          0   \n",
       "0 -57.414993 -57.414993 -57.414993 -57.414993 -57.414993 -57.414993   \n",
       "\n",
       "         6          7          8          9    ...        206        207  \\\n",
       "           0          0          0          0  ...          0          0   \n",
       "0 -57.414993 -57.414993 -57.414993 -57.414993  ... -27.840111 -27.892956   \n",
       "\n",
       "         208        209        210        211        212        213  \\\n",
       "           0          0          0          0          0          0   \n",
       "0 -29.425554 -31.255728 -34.011845 -34.322586 -33.564934 -36.074425   \n",
       "\n",
       "         214        215  \n",
       "           0          0  \n",
       "0 -37.658695 -38.918003  \n",
       "\n",
       "[1 rows x 216 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "livedf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "twodim= np.expand_dims(livedf2, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "1/1 [==============================] - 0s 36ms/step\n"
     ]
    }
   ],
   "source": [
    "\n",
    "livepreds = loaded_model.predict(twodim, \n",
    "                         batch_size=32, \n",
    "                         verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4.1952990e-03, 2.1430026e-03, 3.2129820e-02, 9.1444800e-04,\n",
       "        1.4785588e-04, 2.1268935e-03, 5.3498585e-02, 1.4044413e-01,\n",
       "        7.5640082e-01, 7.9990271e-03]], dtype=float32)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "livepreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "livepreds1=livepreds.argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "liveabc = livepreds1.astype(int).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "livepredictions = (lb.inverse_transform((liveabc)))\n",
    "livepredictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
